{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Early Time Series Classification - Average Ouput KNN.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPU7Jy7OFKq9UPq7WNyL9Rd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Connect Drive"],"metadata":{"id":"XVaAULW6qhh1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DdiqzlkZMhe"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive') "]},{"cell_type":"markdown","metadata":{"id":"ttpluWU4tHLq"},"source":["### Package Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_3NHgGwZIsI"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import pandas as pd\n","import tensorflow as tf\n","from scipy.signal import savgol_filter\n","from collections import Counter\n","from collections import defaultdict\n","from scipy.optimize import curve_fit\n","from scipy.signal import filtfilt\n","from scipy.spatial import distance\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n","from sklearn.neighbors import KNeighborsClassifier\n","import seaborn as sns\n","import pickle"]},{"cell_type":"markdown","source":["### GPU Device"],"metadata":{"id":"6cosBM9Jd74f"}},{"cell_type":"code","source":["!nvidia-smi -L"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kElpooT1fLzz","executionInfo":{"status":"ok","timestamp":1655551629458,"user_tz":-60,"elapsed":368,"user":{"displayName":"Aditya Gupta","userId":"18047065785542078813"}},"outputId":"14c40c2b-55e1-44f8-a78e-976284a2e01d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-19e4dd9f-2cfa-fc78-1ab0-23174ea3a8e2)\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1655551634039,"user":{"displayName":"Aditya Gupta","userId":"18047065785542078813"},"user_tz":-60},"id":"bLu_lZGKu9dp","outputId":"1a7700b4-b0f3-4bb6-8099-f85dc77f7ee2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/device:GPU:0\n"]}],"source":["gpu = tf.test.gpu_device_name()\n","print(gpu)"]},{"cell_type":"markdown","metadata":{"id":"ihJkU1v2STVo"},"source":["### Pre-Processing Helper Functions"]},{"cell_type":"code","source":["def decaying_exp(x, a, b):\n","    \"\"\" Returns exponential function\n","\n","    Parameters\n","    ----------\n","    x : ndarray\n","        times\n","    a : double\n","        t(inf) value\n","    b : double\n","        slope to t=0\n","        \n","    Returns\n","    -------\n","    ndarray\n","        y-axis values of the function\n","    \"\"\"\n","    return a*(1-np.exp(-b * x))\n","\n","\n","def fit_pixels_interpolate(time, X, interpolate_idx):\n","    \"\"\" Interpolates the curves for each pixel\n","\n","    Parameters\n","    ----------\n","    time : ndarray\n","        times\n","    X : ndarray\n","        TxNM array to be interpolated\n","    idx_active : ndarray\n","        NM array specifying pixels that are active\n","    interpolate_idx : int\n","        interpolation is performed until this index\n","\n","    Returns\n","    -------\n","    popt : ndarray\n","        optimal parameters for interpolation of each pixel, with shape 2xNM\n","    \"\"\"\n","    popt = np.zeros((2, X.shape[1]))\n","\n","    # for every pixel\n","    for i in range(X.shape[1]):\n","\n","      data = filtfilt(b=np.ones(10) / 10, a=[1], x=X[:, i])\n","\n","      # Fit the curve (interpolate) to the decaying exponential\n","      try:\n","        popt[:, i], pcov = curve_fit(decaying_exp, time[:interpolate_idx], data[:interpolate_idx], p0=[-10, 0.1])\n","      except:\n","        \n","        popt[:, i] = None\n","\n","    return popt"],"metadata":{"id":"g1nRYshEtGy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def filter_by_drift(df, interpolate_idx):\n","\n","  \"\"\" Filters pixels by their fitting to the drift model\n","  \n","  Parameters\n","  ----------\n","  df : pandas.DataFrame \n","    DataFrame with pixel data that the fitting is applied to\n","  interpolate_idx : int\n","    Interpolation is perfromed until this index\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame with only the data from the active pixels\n","  drfit_avg : numpy.array\n","    Array containing the average drift value at each time stamp\n","\n","  \"\"\"\n","  \n","  popt = fit_pixels_interpolate(np.array(df.index), df.values, interpolate_idx)\n","\n","  drift_avg = np.zeros(df.shape[0])\n","  pix_count = 0\n","  active = np.array(np.zeros(df.shape[1]), dtype=bool)\n","\n","  for idx in range(df.shape[1]):\n","\n","  # check if any of the drift params for the pixel are nan\n","    if(np.isnan(popt[0, idx]) and np.isnan(popt[1, idx])):\n","      active[idx] = False\n","    else:\n","      # if drift params exist then iterate over the values of the index and use these as x values for the drift curve\n","      y_vals = []\n","      for i in df.index:\n","        val = decaying_exp(i, popt[0,idx], popt[1,idx])\n","        y_vals.append(val)\n","      \n","      # subtract the extrapolated drift from the signal\n","      drift_error = np.abs(np.array(df.values[:, idx] - y_vals))\n","      \n","      # only keep pixels with drift error of less than 30mV\n","      if((drift_error < 30).all()):\n","        drift_avg = np.add(drift_avg, np.array(y_vals))\n","        pix_count += 1\n","        active[idx] = True\n","      else:\n","        active[idx] = False\n","\n","  drift_avg/=pix_count\n","\n","  df = df.loc[:, active]\n","\n","  return df, drift_avg"],"metadata":{"id":"R9gzaEl7wE7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1YMbu9bSW5m"},"outputs":[],"source":["def filter_by_vref(X, v_thresh=70):\n","    '''\n","    Identifies active pixels by checking if one of the first 10 derivatives d(i) is > v_thresh\n","    \n","    Parameters\n","    ---------\n","\n","    X : ndarray\n","        Input 2D array (T x NM). T = time samples, NM = total number of pixels\n","    v_thresh : int, optional\n","        Minimum value of the derivative d(i)=X(i+1)-X(i) in mV. Default is 70\n","    Returns\n","    -------\n","    ndarray\n","        1D array of bool with dimension (NM). For each pixel, returns True if, during the first 10 samples,\n","        one of the derivatives is > v_thresh. The derivatives are calculated as d(i) = X(i+1)-X(i)\n","    '''\n","    return (np.diff(X[:10, :], axis=0) > v_thresh).any(axis=0)  # check if one of the first 10 derivatives is >v_thresh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjXkAhKwSgFB"},"outputs":[],"source":["def filter_by_vrange(X, v_range=(100, 900)):\n","    '''\n","    Identifies active pixels by checking that all the values are in v_range\n","\n","    Parameters\n","    ---------\n","    X : ndarray\n","        Input 2D array (T x NM). T = time samples, NM = total number of pixels\n","    v_range : (int, int), optional\n","        tuple containing the minimum and maximum allowable voltage in mV. Default is (100, 900)\n","        \n","    Returns\n","    -------\n","    ndarray\n","        1D array of bool with dimension (NM). For each pixel, returns True if the value is always in v_range\n","    '''\n","    return (X < v_range[1]).all(axis=0) & (X > v_range[0]).all(axis=0)  # for each pixel, check if all the values are\n","    # within the given range\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5a7Uqi9Skg_"},"outputs":[],"source":["def filter_by_derivative(X, vthresh=5):\n","    \"\"\" Identifies active pixels by checking that the absolute value of the derivative is always below vthresh\n","\n","    Parameters\n","    ----------\n","    X : ndarray\n","        input 2D array of shape TxNM\n","    vthresh : int\n","        threshold for active pixels. Default is 5\n","        \n","    Returns\n","    -------\n","    ndarray\n","        1D array of bool with dimension (NM). For each pixel, returns True if all the derivatives are below vthresh\n","    \"\"\"\n","    x_diff = np.abs(np.diff(X, axis=0))\n","    return (x_diff < vthresh).all(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUOV5CRYflUO"},"outputs":[],"source":["def filter_active_pixels(df, v_thresh_ref=50, v_range=(100, 900), v_thresh_deriv=5): \n","  \"\"\" Filters pixels by reference electrode voltage, derivative and voltage range \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be filtered \n","  v_thresh_ref : int, optional\n","    Threshold for active pixels for filtering by reference electrode volatge . Default is 50\n","  v_range : (int, int), optional\n","    Tuple containing the minimum and maximum allowable voltage in mV for the voltage range filteration. Default is (100, 900)\n","  v_thresh_deriv : int, optional\n","    Threshold for filtering pixels by derivative. Default is 5\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after data from inactive pixels is removed\n","  \"\"\"\n","\n","  active = filter_by_vref(df.values, v_thresh_ref) & filter_by_vrange(df.values, v_range) & filter_by_derivative(df.values, v_thresh_deriv)\n","\n","  # drop pixels \n","  df = df.loc[: , active]\n","\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZJkYzPiVvd6"},"outputs":[],"source":["def filter_active_pixels_deriv(df, v_thresh_deriv=5): \n","  \"\"\" Filters pixels by derivative  \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be filtered \n","  v_thresh_deriv : int, optional\n","    Threshold for filtering pixels by derivative. Default is 5\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after data from inactive pixels is removed\n","  \"\"\"\n","\n","  active = filter_by_derivative(df.values, v_thresh_deriv)\n","  \n","  # drop pixels \n","  df = df.loc[: , active]\n","  return df"]},{"cell_type":"code","source":["def filter_active_pixels_range(df, v_range=(100, 900)):\n","  \"\"\" Filters pixels by voltage range \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be filtered \n","  v_range : (int, int), optional\n","    Tuple containing the minimum and maximum allowable voltage in mV for the voltage range filteration. Default is (100, 900)\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after data from inactive pixels is removed\n","  \"\"\"\n","  active = filter_by_vrange(df.values, v_range)\n","\n","  # drop pixels \n","  df = df.loc[: , active]\n","  return df"],"metadata":{"id":"imVXR8eVUrby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reshape_data(df, rows, cols):\n","  \"\"\" Reshapes TxNM data into TxNxM where (T = Number of time samples, N = Number of Rows, M = Number of Columns)\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be reshaped\n","  rows : int\n","    Number of rows \n","  cols : int\n","    Number of columns\n","\n","  Returns\n","  -------\n","  X : numpy.ndarray\n","    3D array containing the reshaped data \n","  \"\"\"\n","\n","  X = df.values #pandas.DataFrame.values: Return a Numpy representation of the DataFrame.\n","  X = X.reshape(-1, rows, cols, order='F') #or C. different reshaping row by row or column by column but this works\n","  return X"],"metadata":{"id":"RTF9Vh78MZSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def filter_chemical_pixels(df, arr_rows, arr_cols):\n","  \"\"\" Removes all the temperature pixels from the data \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will filtered\n","  rows : int\n","    Number of rows \n","  cols : int\n","    Number of columns\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after temperature pixels are removed\n","  \"\"\"\n","\n","  X = reshape_data(df, arr_rows, arr_cols) # reshape data to T x 78 x 56\n","  X_mean = np.mean(X, axis=0) # get mean to have 78 x 56 shape\n","  X_mean[1::3, 1::3] = np.nan # set temperature pixels to nan\n","  X_mean = X_mean.flatten('F') # restore shape to 4068 \n","\n","  active_chemical = ~(np.isnan(X_mean)) # get bool array of all chemical pixels\n","\n","  # drop pixels \n","  df = df.loc[: , active_chemical]\n","  return df\n"],"metadata":{"id":"D9Xt8X4zL7hc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o82EQTYe9euH"},"outputs":[],"source":["def time_to_index(times, time_vect):\n","    '''Returns index of the times closest to the desired ones time_vect\n","\n","    Parameters\n","    ---------\n","    times : list\n","        list of integers containing the desired times\n","    time_vect : nparray\n","        array of the times at which the values are sampled\n","    Returns\n","    -------\n","    list\n","        for each element in the input list times, return an element in the output list\n","        with the index of the sample closest to the desired time\n","    '''\n","    indices = []\n","    for time in times:  # for each time in the input list\n","        indices.append( np.argmin(np.abs(time_vect - time)) )\n","        # find index of the sampled time (in time_vect) closest to the desired one (time)\n","    return indices\n","\n","\n","def find_loading_time(time_vect, X, bounds=(600, 900)):  # for v2\n","    ''' Finds loading and settling time for the data of v2 chip\n","\n","    Parameters\n","    ----------\n","    time_vect : ndarray\n","        1D array with dimension T containing the sampling times\n","    X : ndarray\n","        2D array with dimension TxNM containing the sampled data\n","    bounds : list, optional\n","        tuple containing the minimum and maximum times (in ms) where the loading time has to be searched.\n","        Default is (600, 900)\n","        \n","    Returns\n","    -------\n","    tuple\n","        - settled_index : index at which the settling occurs\n","        - settled_time : time at which the settling occurs\n","    '''\n","\n","    search_start, search_end = time_to_index(bounds, time_vect)  # for each time in bounds, find the index\n","    # of the sample (in time_vect) that is closest to the desired one (in bounds)\n","    X_mean = np.mean(X, axis=1)  # for each sample, calculate the mean of all pixels\n","    X_mean_diff = np.diff(X_mean)  # find the derivative\n","\n","    loading_index = np.argmax(X_mean_diff[search_start:search_end]) + search_start + 1  # find the index\n","    # where the derivative is max in the specified interval\n","    loading_index = loading_index  # add settling time\n","    settled_index = loading_index + 10  # add settling time\n","    settled_time = time_vect[settled_index]  # find the time that index corresponds to\n","\n","    return settled_index, settled_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9m8OqTUtQVb0"},"outputs":[],"source":["def preprocess_data(df, deriv_thresh, deriv_thresh_bgsub=5):\n","  \"\"\"Applies all pre-processing steps to single well experimental data\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will pre-processed\n","  deriv_thresh : int\n","    Threshold for filtering by derivative\n","  deriv_thresh_bgsub : int, optional\n","    Threshold for filtering by derivative after background subtraction step\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame with only data from active pixels after pre-processing\n","  \"\"\"\n","\n","  \n","  df = filter_chemical_pixels(df, 78, 56) # filter all chemical pixels\n","  \n","  df = filter_active_pixels(df=df, v_thresh_deriv=deriv_thresh, v_range=(100,900))\n","\n","  settle_idx, settle_time = find_loading_time(df.index, df, bounds=(600, 900)) # find settling point\n","  df = df.iloc[settle_idx + 10:, :] # use only the data after the settling time + 30s to allow reaction to settle\n","\n","  df = df.sub(df.iloc[0, :], axis='columns') # subtract value of first pixel from all pixels\n","\n","  if(len(filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub).columns) != 0): # check if there is still data present after filtering\n","    df = filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub) # if data is present do filtering otherwise don't\n","\n","  df = df.iloc[0:150+250, :] # take only 400 samples after settling point (approx 19-20mins) \n","  \n","  df.index = df.index - df.index[0] # set the first time value to 0\n","  \n","  X, drift = filter_by_drift(df, 40) # filter by fitting of pixel to drift model\n","\n","  if(len(X.columns) != 0): \n","    df = X\n","\n","  df['Average Output'] = df.mean(axis=1) # compute the mean value after filtering inactive pixels \n","\n","  df['Average Drift'] = drift # add new column for average drift\n","\n","  if(len(X.columns) != 0):\n","    df['Average Output'] = df['Average Output'] - drift # drift compensation \n","\n","  df['Average Output'] = savgol_filter(df['Average Output'],101, 3) # filter to smooth out the noise in the data\n","   \n","  return df"]},{"cell_type":"code","source":["def preprocess_partial_data(df, deriv_thresh, deriv_thresh_bgsub=5):\n","  \"\"\"Applies all pre-processing steps to double well experimental data\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will pre-processed\n","  deriv_thresh : int\n","    Threshold for filtering by derivative\n","  deriv_thresh_bgsub : int, optional\n","    Threshold for filtering by derivative after background subtraction step\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame with only data from active pixels after pre-processing\n","  \"\"\"\n","\n","  df = filter_active_pixels_range(df=df, v_range=(100,900)) # filter by range incase of any saturation\n","  \n","  df = filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh) # filter pixels by deriv\n","\n","  df = df.sub(df.iloc[0, :], axis='columns') # subtract value of first pixel from all pixels\n","\n","  if(len(filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub).columns) != 0): # check if there is still data present after filtering\n","    df = filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub) # if data is present do filtering otherwise dont\n","\n","  df = df.iloc[0:150+250, :] # take only 400 samples after settling point (approx 19-20mins)\n","  \n","  df.index = df.index - df.index[0] # set the first time value to 0\n","\n","  X, drift = filter_by_drift(df, 40) # filter by fitting of pixel to drift model\n","\n","  if(len(X.columns) != 0):\n","    df = X\n","  \n","  df['Average Output'] = df.mean(axis=1) # compute the mean value after filtering inactive pixels \n","\n","  df['Average Drift'] = drift # average drift column\n","\n","  if(len(X.columns) != 0):\n","    df['Average Output'] = df['Average Output'] - drift # drift compensation\n","\n","  df['Average Output'] = savgol_filter(df['Average Output'],101, 3) # filter to smooth out the noise in the data\n","    \n","  return df"],"metadata":{"id":"JsSdU8xPZX4U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Loading Helper Functions"],"metadata":{"id":"Dvvp28miEMsF"}},{"cell_type":"code","source":["def load_partial_covid_exp(filepath):\n","  \"\"\" Loading in double well experimental data from csv file\n","\n","  Parameters\n","  ----------\n","  filepath : string\n","    Path to the csv file that loads the double well data\n","  \n","  Returns\n","  -------\n","  df_pos : pandas.DataFrame\n","    DataFrame with pixel data from the positive well \n","  df_neg : panads.DataFrame\n","    DataFrame with pixel data from the negative well\n","  \"\"\"\n","  \n","  bot_filepath = filepath[:-4] + \"_bot.csv\"\n","  top_filepath = filepath[:-4] + \"_top.csv\"\n","\n","  ## load in 2 sheets\n","  df_neg = pd.read_csv(top_filepath, header=0, index_col=0)\n","  df_pos = pd.read_csv(bot_filepath, header=0, index_col=0)\n","\n","  return df_pos, df_neg"],"metadata":{"id":"vL28HCTcZUUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_data(filepath, dictionary):\n","  \"\"\" Saves a dictionary to a pickle file\n","\n","  Parameters\n","  ----------\n","  filepath : string\n","    Path to the pickle file that saves the dictionary\n","  dictionary : dict(panadas.DataFrame)\n","    Dictionary of DataFrames that will be saved\n","  \"\"\"\n","  with open(filepath, 'wb') as f:\n","      pickle.dump(dictionary, f)"],"metadata":{"id":"ZceMjEMIB5jO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(filepath): \n","  \"\"\" Loads a dictionary from a pickle file\n","\n","  Parameters\n","  ----------\n","  filepath : string\n","    Path to the pickle file that loads the dictionary\n","\n","  Returns\n","  -------\n","  loaded_dict : dict(panadas.DataFrame)\n","    Dictionary of DataFrames that is loaded from the file\n","  \"\"\"\n","\n","  with open(filepath, 'rb') as f:\n","      loaded_dict = pickle.load(f)\n","  return loaded_dict"],"metadata":{"id":"TXGrMgxzCbji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation Metric Helper Functions"],"metadata":{"id":"PaNIFO5iSa9C"}},{"cell_type":"code","source":["def accuracy(classifications):\n","  \"\"\" Returns the value of the accuracy from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  accuracy : double\n","    Classification Accuracy \n","  \"\"\"\n","  total = len(classifications)\n","  total_correct = 0\n","  for i in classifications.values():\n","    \n","    if(i[0] == None or i[1] == None): ## if any predictions are inconclusive \n","      continue\n","\n","    if(i[0] == i[1]):\n","      total_correct +=1\n","\n","  accuracy = (total_correct/total)\n","\n","  return accuracy"],"metadata":{"id":"U2zoSqPJLatm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sensitivity(classifications):\n","  \"\"\" Returns the value of the sensitivity from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  sensitivity : double\n","    Classification sensitivity \n","  \"\"\"\n","  true_pos = 0\n","  false_neg = 0\n","\n","  for i in classifications.values():\n","\n","    true_label = int(i[1])\n","    predicted = int(i[0])\n","\n","    if(true_label == 1 and predicted == 1):\n","      true_pos += 1\n","    \n","    if(true_label == 1 and predicted == 0):\n","      false_neg += 1\n","\n","  sensitivity = (true_pos/(true_pos + false_neg))\n","\n","  return sensitivity"],"metadata":{"id":"lzSAF5WsTIuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def specificity(classifications):\n","  \"\"\" Returns the value of the specificity from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  specificity : double\n","    Classification specificity \n","  \"\"\"\n","\n","  true_neg = 0\n","  false_pos = 0\n","\n","  for i in classifications.values():\n","    true_label = int(i[1])\n","    predicted = int(i[0])\n","    \n","    if(true_label == 0 and predicted == 0):\n","      true_neg += 1\n","    \n","    if(true_label == 0 and predicted == 1):\n","      false_pos += 1\n","\n","  specificity = (true_neg/(true_neg + false_pos))\n","\n","  return specificity"],"metadata":{"id":"WP_kdiXMYeU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def precision(classifications):\n","  \"\"\" Returns the value of the precision from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  precision : double\n","    Classification precision \n","  \"\"\"\n","  true_pos = 0\n","  false_pos = 0\n","\n","  for i in classifications.values():\n","    true_label = int(i[1])\n","    predicted = int(i[0])\n","    \n","    if(true_label == 1 and predicted == 1):\n","      true_pos += 1\n","    \n","    if(true_label == 0 and predicted == 1):\n","      false_pos += 1\n","\n","  precision = (true_pos/(true_pos + false_pos))\n","\n","  return precision"],"metadata":{"id":"w7-_ZPDDaxRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f1(classifications):\n","  \"\"\" Returns the value of the F1 score from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  double\n","    Classification F1 score \n","  \"\"\"\n","  numerator = 2*precision(classifications)*sensitivity(classifications)\n","  denominator = precision(classifications) + sensitivity(classifications)\n","  return numerator/denominator"],"metadata":{"id":"qkFpU-UJbV1R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Array Dims"],"metadata":{"id":"W9kgS_-Cx1nm"}},{"cell_type":"code","source":["arr_rows = 78\n","arr_cols = 56"],"metadata":{"id":"whsJZh4Zx0xs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"KCr7gvB_tf5-"}},{"cell_type":"markdown","source":["#### Positive Samples"],"metadata":{"id":"AvJiLnQ8tiKx"}},{"cell_type":"code","source":["## Average pixel value for all samples \n","\n","with tf.device(gpu):\n","  ## Gamma 1\n","  avg_data_g1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma1.app.1e5/gamma1.app.1e5_data_export.csv\"\n","  avg_g1 = pd.read_csv(avg_data_g1_file, header=0)\n","\n","  ## Gamma 2\n","  avg_data_g2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma2.app.1e4/gamma2.app.1e4_data_export.csv\"\n","  avg_g2 = pd.read_csv(avg_data_g2_file, header=0)\n","\n","  ## Gamma 3\n","  avg_data_g3_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma3.app.1e5/gamma3.app.1e5_data_export.csv\"\n","  avg_g3 = pd.read_csv(avg_data_g3_file, header=0)\n","  \n","  ## Gamma 5 \n","  avg_data_g5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma5.app.1e4/gamma5.app.1e4_data_export.csv\"\n","  avg_g5 = pd.read_csv(avg_data_g5_file, header=0)\n","\n","  ## 22RV1.ap1\n","  avg_data_22rv1_ap1_file = \"/DNAPositives/22RV1.ap1/22RV1.ap1_data_export.csv\"\n","  avg_22rv1_ap1 = pd.read_csv(avg_data_22rv1_ap1_file, header=0)\n","\n","  ## 22RV1.ap2\n","  avg_data_22rv1_ap2_file = \"/DNAPositives/22RV1.ap2/22RV1.ap2_data_export.csv\"\n","  avg_22rv1_ap2 = pd.read_csv(avg_data_22rv1_ap2_file, header=0)\n","\n","  ## 22RV1y.p3\n","  avg_data_22rv1y_p3_file = \"/DNAPositives/22Rv1y.p3/22Rv1y.p3_data_export.csv\"\n","  avg_22rv1y_p3 = pd.read_csv(avg_data_22rv1y_p3_file, header=0)\n","\n","  ## 22RV1y.p4\n","  avg_data_22rv1y_p4_file = \"/DNAPositives/22Rv1y.p4/22Rv1y.p4_data_export.csv\"\n","  avg_22rv1y_p4 = pd.read_csv(avg_data_22rv1y_p4_file, header=0)\n","\n","  ## ARV7.p1\n","  avg_data_arv7_p1_file = \"/DNAPositives/ARV7.p1/ARV7.p1_data_export.csv\"\n","  avg_arv7_p1 = pd.read_csv(avg_data_arv7_p1_file, header=0).iloc[1:, :].reset_index(drop=True) # row 0 was NAN\n","\n","  ## ARV7.p3\n","  avg_data_arv7_p3_file = \"/DNAPositives/ARV7.p3/ARV7.p3_data_export.csv\"\n","  avg_arv7_p3 = pd.read_csv(avg_data_arv7_p3_file, header=0)\n","\n","  ## ARV7.p4\n","  avg_data_arv7_p4_file = \"/DNAPositives/ARV7.p4/ARV7.p4_data_export.csv\"\n","  avg_arv7_p4 = pd.read_csv(avg_data_arv7_p4_file, header=0)\n","\n","  ## Beta 1\n","  avg_data_b1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta1.app.1e4/beta1.app.1e4_data_export.csv\"\n","  avg_b1 = pd.read_csv(avg_data_b1_file, header=0)\n","\n","  ## Beta 2\n","  avg_data_b2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta2.app.1e5/beta2.app.1e5_data_export.csv\"\n","  avg_b2 = pd.read_csv(avg_data_b2_file, header=0)\n","\n","  ## Beta 5\n","  avg_data_b5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta5.app.1e5/beta5.app.1e5_data_export.csv\"\n","  avg_b5 = pd.read_csv(avg_data_b5_file, header=0)\n","  "],"metadata":{"id":"Ekqd_pB0tuTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## All pixel values for each time stamp\n","\n","with tf.device(gpu):\n","  ## Gamma 1\n","  g1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma1.app.1e5/gamma1.app.1e5_vsChem_export.csv\"\n","  g1 = pd.read_csv(g1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g1.index = avg_g1[\"Time Elapsed\"]\n","\n","  ## Gamma 2\n","  g2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma2.app.1e4/gamma2.app.1e4_vsChem_export.csv\"\n","  g2 = pd.read_csv(g2_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g2.index = avg_g2[\"Time Elapsed\"]\n","\n","  ## Gamma 3\n","  g3_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma3.app.1e5/gamma3.app.1e5_vsChem_export.csv\"\n","  g3 = pd.read_csv(g3_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g3.index = avg_g3[\"Time Elapsed\"]\n","\n","  ## Gamma 5\n","  g5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma5.app.1e4/gamma5.app.1e4_vsChem_export.csv\"\n","  g5 = pd.read_csv(g5_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g5.index = avg_g5[\"Time Elapsed\"]\n","\n","  ## 22RV1.ap1\n","  rv1_ap1_file = \"/DNAPositives/22RV1.ap1/22RV1.ap1_vsChem_export.csv\"\n","  rv1_ap1 = pd.read_csv(rv1_ap1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1_ap1.index = avg_22rv1_ap1['Time Elapsed']\n","\n","  ## 22RV1.ap2\n","  rv1_ap2_file = \"/DNAPositives/22RV1.ap2/22RV1.ap2_vsChem_export.csv\"\n","  rv1_ap2 = pd.read_csv(rv1_ap2_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1_ap2.index = avg_22rv1_ap2['Time Elapsed']\n","\n","  ## 22RV1y.p3\n","  rv1y_p3_file = \"/DNAPositives/22Rv1y.p3/22Rv1y.p3_vsChem_export.csv\"\n","  rv1y_p3 = pd.read_csv(rv1y_p3_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1y_p3.index = avg_22rv1y_p3['Time Elapsed']\n","\n","  ## 22RV1y.p4\n","  rv1y_p4_file = \"/DNAPositives/22Rv1y.p4/22Rv1y.p4_vsChem_export.csv\"\n","  rv1y_p4 = pd.read_csv(rv1y_p4_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1y_p4.index = avg_22rv1y_p4['Time Elapsed']\n","\n","  ## ARV7.p1 \n","  arv7_p1_file = \"/DNAPositives/ARV7.p1/ARV7.p1_vsChem_export.csv\"\n","  arv7_p1 = pd.read_csv(arv7_p1_file, header=None).iloc[:, :(arr_rows*arr_cols)] \n","  arv7_p1.index = avg_arv7_p1[\"Time Elapsed\"]\n","\n","  ## ARV7.p3 \n","  arv7_p3_file = \"/DNAPositives/ARV7.p3/ARV7.p3_vsChem_export.csv\"\n","  arv7_p3 = pd.read_csv(arv7_p3_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv7_p3.index = avg_arv7_p3[\"Time Elapsed\"]\n","\n","  ## ARV7.p4 \n","  arv7_p4_file = \"/DNAPositives/ARV7.p4/ARV7.p4_vsChem_export.csv\"\n","  arv7_p4 = pd.read_csv(arv7_p4_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv7_p4.index = avg_arv7_p4[\"Time Elapsed\"]\n","\n","  ## Beta 1\n","  b1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta1.app.1e4/beta1.app.1e4_vsChem_export.csv\"\n","  b1 = pd.read_csv(b1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  b1.index = avg_b1[\"Time Elapsed\"]\n","\n","  ## Beta 2\n","  b2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta2.app.1e5/beta2.app.1e5_vsChem_export.csv\"\n","  b2 = pd.read_csv(b2_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  b2.index = avg_b2[\"Time Elapsed\"]\n","\n","  ## Beta 5\n","  b5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta5.app.1e5/beta5.app.1e5_vsChem_export.csv\"\n","  b5 = pd.read_csv(b5_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  b5.index = avg_b5[\"Time Elapsed\"]"],"metadata":{"id":"vZRah5zpxXp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Negative Samples"],"metadata":{"id":"7qOF9VBstkbe"}},{"cell_type":"code","source":["## Average pixel value for all samples \n","\n","with tf.device(gpu):  \n","  ## ARV7.n1\n","  avg_data_arv7_file = \"/DNANegatives/ARV7.n1/ARV7.n1_data_export.csv\"\n","  avg_arv7 = pd.read_csv(avg_data_arv7_file, header=0)\n","\n","  ## Yap.n2\n","  avg_data_yap_file = \"/DNANegatives/yap.n2/yap.n2_data_export.csv\"\n","  avg_yap = pd.read_csv(avg_data_yap_file, header=0)\n","\n","  ## Yap1.n2\n","  avg_data_yap1_file = \"/DNANegatives/yap1.n2/yap1.n2_data_export.csv\"\n","  avg_yap1 = pd.read_csv(avg_data_yap1_file, header=0).iloc[1:, :].reset_index() # row 0 was NAN\n","\n","  ## Yap1.n1.1 \n","  avg_data_yap1n1_file = \"/DNANegatives/yap1.n1.1/yap1.n1.1_data_export.csv\"\n","  avg_yap1n1 = pd.read_csv(avg_data_yap1n1_file, header=0).iloc[1:, :].reset_index() # row 0 was NAN\n","\n","  ## ARV7.n2\n","  avg_data_arv72_file = \"/DNANegatives/ARV7.n2/ARV7.n2_data_export.csv\"\n","  avg_arv72 = pd.read_csv(avg_data_arv72_file, header=0)\n","\n","  ## ARV7.n3\n","  avg_data_arv73_file = \"/DNANegatives/ARV7.n3/ARV7.n3_data_export.csv\"\n","  avg_arv73 = pd.read_csv(avg_data_arv73_file, header=0)\n","\n","  ## DU145y.n1\n","  avg_data_du145y_n1_file = \"/DNANegatives/DU145y.n1/DU145y.n1_data_export.csv\"\n","  avg_du145y_n1 = pd.read_csv(avg_data_du145y_n1_file, header=0)"],"metadata":{"id":"mlU83yKsuSHV","executionInfo":{"status":"ok","timestamp":1655551721441,"user_tz":-60,"elapsed":409,"user":{"displayName":"Aditya Gupta","userId":"18047065785542078813"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["## All pixel values for each time stamp\n","\n","with tf.device(gpu):   \n","  ## ARV7.n1 \n","  arv7_file = \"/DNANegatives/ARV7.n1/ARV7.n1_vsChem_export.csv\"\n","  arv7 = pd.read_csv(arv7_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv7.index = avg_arv7[\"Time Elapsed\"]\n","\n","  ## Yap.n2\n","  yap_file = \"/DNANegatives/yap.n2/yap.n2_vsChem_export.csv\"\n","  yap = pd.read_csv(yap_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  yap.index = avg_yap[\"Time Elapsed\"]\n","\n","  ## Yap1.n2\n","  yap1_file = \"/DNANegatives/yap1.n2/yap1.n2_vsChem_export.csv\"\n","  yap1 = pd.read_csv(yap1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  yap1.index = avg_yap1[\"Time Elapsed\"]\n","\n","  ## Yap1.n1.1\n","  yap1n1_file = \"/DNANegatives/yap1.n1.1/yap1.n1.1_vsChem_export.csv\"\n","  yap1n1 = pd.read_csv(yap1n1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  yap1n1.index = avg_yap1n1[\"Time Elapsed\"]\n","\n","  ## ARV7.n2\n","  arv72_file = \"/DNANegatives/ARV7.n2/ARV7.n2_vsChem_export.csv\"\n","  arv72 = pd.read_csv(arv72_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv72.index = avg_arv72[\"Time Elapsed\"]\n","\n","  ## ARV7.n3\n","  arv73_file = \"/DNANegatives/ARV7.n3/ARV7.n3_vsChem_export.csv\"\n","  arv73 = pd.read_csv(arv73_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv73.index = avg_arv73[\"Time Elapsed\"]\n","\n","  ## DU145y.n1\n","  du145y_n1_file = \"/DNANegatives/DU145y.n1/DU145y.n1_vsChem_export.csv\"\n","  du145y_n1 = pd.read_csv(du145y_n1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  du145y_n1.index = avg_du145y_n1[\"Time Elapsed\"]"],"metadata":{"id":"W3_XExOjypwI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Partial Covid Data"],"metadata":{"id":"yjXPLEfmRUJH"}},{"cell_type":"code","source":["## 150520_2_118\n","avg_118_file = \"/COVIDPartialData/150520_2_118/exp_summary_118.csv\"\n","exp_118_pos, exp_118_neg = load_partial_covid_exp(avg_118_file)\n","\n","## 150520_4_2_86\n","avg_86_file = \"/COVIDPartialData/150520_4_2_86/exp_summary_86.csv\"\n","exp_86_pos, exp_86_neg = load_partial_covid_exp(avg_86_file)\n","\n","## 150520_5_129\n","avg_129_file = \"/COVIDPartialData/150520_5_129/exp_summary_129.csv\"\n","exp_129_pos, exp_129_neg = load_partial_covid_exp(avg_129_file)\n","\n","## 180520_4_165\n","avg_165_file = \"/COVIDPartialData/180520_4_165/exp_summary_165.csv\"\n","exp_165_pos, exp_165_neg = load_partial_covid_exp(avg_165_file)\n","\n","## 180520_6_35\n","avg_35_file = \"/COVIDPartialData/180520_6_35/exp_summary_35.csv\"\n","exp_35_pos, exp_35_neg = load_partial_covid_exp(avg_35_file)\n","\n","## 190520_1_28\n","avg_28_file = \"/COVIDPartialData/190520_1_28/exp_summary_28.csv\"\n","exp_28_pos, exp_28_neg = load_partial_covid_exp(avg_28_file) \n","\n","## 190520_2_14\n","avg_14_file = \"/COVIDPartialData/190520_2_14/exp_summary_14.csv\"\n","exp_14_pos, exp_14_neg = load_partial_covid_exp(avg_14_file)\n","\n","## 210520_2_40\n","avg_40_file = \"/COVIDPartialData/210520_2_40/exp_summary_40.csv\"\n","exp_40_pos, exp_40_neg = load_partial_covid_exp(avg_40_file)\n","\n","## 210520_3_88\n","avg_88_file = \"/COVIDPartialData/210520_3_88/exp_summary_88.csv\"\n","exp_88_pos, exp_88_neg = load_partial_covid_exp(avg_88_file)\n","\n","## 210520_6_27\n","avg_27_file = \"/COVIDPartialData/210520_6_27/exp_summary_27.csv\"\n","exp_27_pos, exp_27_neg = load_partial_covid_exp(avg_27_file)\n","\n","## 250520_1_134\n","avg_134_file = \"/COVIDPartialData/250520_1_134/exp_summary_134.csv\"\n","exp_134_pos, exp_134_neg = load_partial_covid_exp(avg_134_file)\n","\n","## 250520_2_97\n","avg_97_file = \"/COVIDPartialData/250520_2_97/exp_summary_97.csv\"\n","exp_97_pos, exp_97_neg = load_partial_covid_exp(avg_97_file)\n","\n","## 250520_6_2D1\n","avg_2d1_file = \"/COVIDPartialData/250520_6_2D1/exp_summary_2D1.csv\"\n","exp_2d1_pos, exp_2d1_neg = load_partial_covid_exp(avg_2d1_file)\n","\n","## 250520_7_64\n","avg_64_file = \"/COVIDPartialData/250520_7_64/exp_summary_64.csv\"\n","exp_64_pos, exp_64_neg = load_partial_covid_exp(avg_64_file)"],"metadata":{"id":"ORRtMFfEZBwV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"7XgnkewwwPki"}},{"cell_type":"markdown","source":["#### Positive Samples"],"metadata":{"id":"CTcUwvRiwUmJ"}},{"cell_type":"code","source":["g1 = preprocess_data(g1, 500)\n","g2 = preprocess_data(g2, 500)\n","g3 = preprocess_data(g3, 500)\n","g5 = preprocess_data(g5, 500)\n","rv1_ap1 = preprocess_data(rv1_ap1, 500)\n","rv1_ap2 = preprocess_data(rv1_ap2, 500)\n","rv1y_p3 = preprocess_data(rv1y_p3, 500)\n","rv1y_p4 = preprocess_data(rv1y_p4, 500)\n","arv7_p1 = preprocess_data(arv7_p1, 500)\n","arv7_p3 = preprocess_data(arv7_p3, 500)\n","arv7_p4 = preprocess_data(arv7_p4, 500)\n","b1 = preprocess_data(b1, 500)\n","b2 = preprocess_data(b2, 500)\n","b5 = preprocess_data(b5, 500)"],"metadata":{"id":"1-WlDoK49D2Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Negative Samples"],"metadata":{"id":"1WaPBFGuwYN4"}},{"cell_type":"code","source":["arv7 = preprocess_data(arv7, 500)\n","yap = preprocess_data(yap, 500)\n","yap1 = preprocess_data(yap1, 500)\n","yap1n1 = preprocess_data(yap1n1, 500)\n","arv72 = preprocess_data(arv72, 500)\n","arv73 = preprocess_data(arv73, 500)\n","du145y_n1 = preprocess_data(du145y_n1, 500)"],"metadata":{"id":"gazhgzLT9HLV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Covid Partial Data"],"metadata":{"id":"nUwBPNQNjQ7w"}},{"cell_type":"code","source":["exp_118_pos = preprocess_partial_data(exp_118_pos, 500)\n","exp_86_pos = preprocess_partial_data(exp_86_pos, 500)\n","exp_129_pos = preprocess_partial_data(exp_129_pos, 500)\n","exp_165_pos = preprocess_partial_data(exp_165_pos, 500)\n","exp_35_pos = preprocess_partial_data(exp_35_pos, 500)\n","exp_28_pos = preprocess_partial_data(exp_28_pos, 500)\n","exp_14_pos = preprocess_partial_data(exp_14_pos, 500)\n","exp_40_pos = preprocess_partial_data(exp_40_pos, 500)\n","exp_88_pos = preprocess_partial_data(exp_88_pos, 500)\n","exp_27_pos = preprocess_partial_data(exp_27_pos, 500)\n","exp_134_pos = preprocess_partial_data(exp_134_pos, 500)\n","exp_97_pos = preprocess_partial_data(exp_97_pos, 500)\n","exp_2d1_pos = preprocess_partial_data(exp_2d1_pos, 500)\n","exp_64_pos = preprocess_partial_data(exp_64_pos, 500)"],"metadata":{"id":"HQBQ_1YF9Oqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exp_118_neg = preprocess_partial_data(exp_118_neg, 500)\n","exp_86_neg = preprocess_partial_data(exp_86_neg, 500)\n","exp_129_neg = preprocess_partial_data(exp_129_neg, 500)\n","exp_165_neg = preprocess_partial_data(exp_165_neg, 500)\n","exp_35_neg = preprocess_partial_data(exp_35_neg, 500)\n","exp_28_neg = preprocess_partial_data(exp_28_neg, 500)\n","exp_14_neg = preprocess_partial_data(exp_14_neg, 500)\n","exp_40_neg = preprocess_partial_data(exp_40_neg, 500)\n","exp_88_neg = preprocess_partial_data(exp_88_neg, 500)\n","exp_27_neg = preprocess_partial_data(exp_27_neg, 500)\n","exp_134_neg = preprocess_partial_data(exp_134_neg, 500)\n","exp_97_neg = preprocess_partial_data(exp_97_neg, 500)\n","exp_2d1_neg = preprocess_partial_data(exp_2d1_neg, 500)\n","exp_64_neg = preprocess_partial_data(exp_64_neg, 500)"],"metadata":{"id":"sYsOnsAW9Rob"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Machine Learning - KNN Ensemble"],"metadata":{"id":"GvAKajynLNa9"}},{"cell_type":"markdown","source":["#### Helper Functions"],"metadata":{"id":"H8ueQFk24bbg"}},{"cell_type":"code","source":["def get_training_data_knn(positive_samples, negative_samples, timestamp, test_samples=[]):\n","  \"\"\" Gets the training data for the KNN classifier at a given timestamp\n","\n","  Parameters\n","  ----------\n","  positive_samples : dict(pandas.DataFrame)\n","    Dictionary of DataFrames with data from all the positive experiements\n","  negative_samples : dict(pandas.DataFrame)\n","    Dictionary of DataFrames with data from all the negative experiements\n","  timestamp : int\n","    Number of data-points which the data for each experiement must be truncated to\n","  test_samples : array, optional\n","    Array containing the name (key used in the dictionary) of the test samples, Default is []\n","\n","  Returns\n","  -------\n","  numpy.ndarray\n","    Contains the training data for the classifier at the given time stamp\n","  training_labels : numpy.array\n","    1D array with the true labels for each of the training experiements\n","  \"\"\"\n","\n","  training_data = []\n","  pos_count = 0\n","  neg_count = 0\n","\n","\n","  ## iterate postive samples dict\n","  for key, sample in positive_samples.items():\n","\n","    ## if dataset is test data do not add to training set\n","    if(key in test_samples):\n","      continue\n","\n","    ## truncate sample to length t = timestamp\n","    pos_subsample = sample['Average Output'].to_numpy()[0:timestamp]\n","\n","    ## append subsample of length t to training data\n","    training_data.append(pos_subsample)\n","    pos_count += 1\n","\n","\n","  ## iterate negative samples dict\n","  for key, sample in negative_samples.items():\n","\n","    ## if dataset is test data do not add to training set\n","    if(key in test_samples):\n","      continue\n","\n","    ## truncate sample to length t = timestamp\n","    neg_subsample = sample['Average Output'].to_numpy()[0:timestamp]\n","\n","    ## append subsample of length t to training data\n","    training_data.append(neg_subsample)\n","    neg_count += 1\n","\n","  ## create positive and negative (1 and 0) label based on sample \n","  pos_labels = np.ones(pos_count)\n","  neg_labels = np.zeros(neg_count)\n","\n","  ## concatenate labels for final training labels\n","  training_labels = np.concatenate((pos_labels, neg_labels), axis=0)\n","\n","  return np.asarray(training_data), training_labels ## np.asarry() converts list to 2D np array"],"metadata":{"id":"x4q6d44BLwpQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_test_data_knn(sample, timestamp):\n","  \"\"\" Gets the test data for the KNN classifier at a given timestamp for a test sample\n","\n","  Parameters\n","  ----------\n","  sample : pandas.DataFrame\n","    The test sample to be used \n","  timestamp : int\n","    Number of data-points which the data for each experiement must be truncated to\n","\n","  Returns\n","  -------\n","  numpy.ndarray\n","    Contains the test data for the classifier at the given time stamp\n","  \"\"\"\n","  subsample = []\n","  subsample.append(sample['Average Output'].to_numpy()[0:timestamp])\n","\n","  return np.asarray(subsample)"],"metadata":{"id":"GZOfy-0GQuGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_time_index(timestamps, predictions):\n","  \"\"\" Get the timestamp (in terms of number of data points) where the majority vote has been achived for an ensmeble of classifiers\n","\n","  Parameters\n","  ----------\n","  timestamps : array\n","    Array of ints with timestamps when each classification is made\n","  predictions : array\n","    Array containing the prediction made by the classifiers at each of the timestamps\n","\n","  Returns\n","  -------\n","    int\n","      The number of data-points after which the majority vote is determined for an ensemble of classifiers \n","  \"\"\"\n","\n","  ## create dict to hold count of predictions\n","  label_counters = defaultdict(int)\n","\n","  ## add entries to dict\n","  for index, pred in enumerate(predictions):\n","    label_counters[pred] += 1\n","\n","    ## if label count == half of total possible predictions then majority is achieved\n","    if(label_counters[pred] == int(len(predictions)/2)+1):\n","      return timestamps[index]\n","  \n","  return -1"],"metadata":{"id":"2hib6StpHWbU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Training Data"],"metadata":{"id":"FOMhNuiKNGAw"}},{"cell_type":"code","source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"],"metadata":{"id":"-prfZYD_VgMB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Timestamps"],"metadata":{"id":"Ry9pqKjnNKiI"}},{"cell_type":"code","source":["number_of_samples = len(g1['Average Output'])\n","number_of_timestamps = 50\n","\n","timestep = int(number_of_samples/number_of_timestamps)\n","timestamps = [*range(timestep, number_of_samples+timestep, timestep)]"],"metadata":{"id":"3cDeyMc3M_bN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Model"],"metadata":{"id":"DJBWoo1SNMy3"}},{"cell_type":"code","source":["def KNN(k, test_sample, train_data, train_labels, distance_metric):\n","\n","  \"\"\" Generates classification output for a test sample using the KNN classifier\n","\n","  Parameters\n","  ----------\n","  k : int \n","    The value of the hyper-parameter k in the KNN classifier\n","  test_sample : numpy.ndarray\n","    The test sample that the prediction is made for\n","  training_data : numpy.ndarray\n","    Training data used for classification\n","  train_labels : numpy.array\n","    Training labels used for classification\n","  distance_metric : string\n","    Distance metric used for the KNN classifier\n","\n","  Returns\n","  -------\n","  final_pred : double\n","    Final classfication for the test sample\n","  \"\"\" \n","\n","  distances = None\n","\n","  if(distance_metric.lower() == 'manhattan' or distance_metric.lower() == 'cityblock'):\n","    distances = manhattan_distances(test_sample, train_data) # get pair wise manhattan distance for every row\n","  elif(distance_metric.lower() == 'euclidean'):\n","    distances = euclidean_distances(test_sample, train_data) # get pair wise euclidean distance for every row \n","  elif(distance_metric.lower() == 'cosine'):\n","    distances = cosine_distances(test_sample, train_data) # get pair wise cosine distance for every row \n","\n","  distances = np.squeeze(distances, axis=0) # remove redundant dimension\n","\n","  min_indexes = np.argsort(distances)[:k] # get k smallest indexes\n","\n","  knn_labels = list(train_labels[min_indexes]) # get k predictions\n","\n","  final_pred = max(set(knn_labels), key=knn_labels.count)\n","\n","  return final_pred"],"metadata":{"id":"sGJZphgaLeL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def KNN_auto(k, test_sample, train_data, train_labels, distance_metric):\n","  \"\"\" Generates classification output for a test sample using the KNN classifier using the scikit-learn implementation\n","\n","  Parameters\n","  ----------\n","  k : int \n","    The value of the hyper-parameter k in the KNN classifier\n","  test_sample : numpy.ndarray\n","    The test sample that the prediction is made for\n","  training_data : numpy.ndarray\n","    Training data used for classification\n","  train_labels : numpy.array\n","    Training labels used for classification\n","  distance_metric : string\n","    Distance metric used for the KNN classifier\n","\n","  Returns\n","  -------\n","  final_pred : double\n","    Final classfication for the test sample\n","  \"\"\" \n","  KnnClassifier = KNeighborsClassifier(n_neighbors=k, weights=\"uniform\")\n","  KnnClassifier.fit(X=train_data, y=train_labels)\n","  # dist, ind = KnnClassifier.kneighbors(X=test_sample, n_neighbors=k, return_distance=True)\n","  pred = KnnClassifier.predict(X=test_sample)\n","  \n","  return pred[0] # pred is of type np.ndarray with one value so return indx 0 to get raw value\n"],"metadata":{"id":"pSc6qhCylWYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Model Predictions"],"metadata":{"id":"0C0XQKFHFtFV"}},{"cell_type":"markdown","source":["##### Cross Validation"],"metadata":{"id":"DuKAj66EbskM"}},{"cell_type":"code","source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"],"metadata":{"id":"RGVdwT8rxS23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## combine positive and negative sample dicts\n","all_samples = {}\n","all_samples.update(positives)\n","all_samples.update(negatives)\n","\n","## create dict of samples with true label\n","keys = list(all_samples.keys())\n","true_labels = list(np.concatenate((np.ones(len(positives)),np.zeros(len(negatives)))))\n","true_label_dict = dict(zip(keys, true_labels))"],"metadata":{"id":"SUDmHpF-GigC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  final_classifications = {}\n","  final_predictions = []\n","  final_TTP = []\n","  prediction_correctness = []\n","\n","  ## use KNN to evaluate the prediction for each of the samples individually\n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    print(f\"Testing sample {test_sample_name}\")\n","\n","    ## generate predictions for sample using KNN ensemble\n","    predictions = []\n","    for t in timestamps:\n","      train_data, train_labels = get_training_data_knn(positive_samples=positives, negative_samples=negatives, timestamp=t, test_samples=[test_sample_name])\n","      test_data = get_test_data_knn(test_sample, t)\n","      pred = KNN(3, test_data, train_data, train_labels, 'cosine')\n","      predictions.append(pred)\n","    \n","    ## get time to result in seconds using majority voting\n","    time_index = get_time_index(timestamps, predictions) # get the value of the sample at which the sample needs to be indexed\n","    time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time acorrding the experiment at which result is obtained\n","\n","    ## get final prediction\n","    classification = Counter(predictions).most_common(1)[0][0] # final prediction\n","\n","    ## update arrays \n","    final_classifications[key] = (classification, true_label_dict[key])\n","    final_predictions.append(classification) \n","    prediction_correctness.append(\"Yes\" if classification == true_label_dict[key] else \"No\")\n","    print(f\"Predicted Label: {classification} \\t True Label: {true_label_dict[key]} \\t Correct?: {classification == true_label_dict[key]}\")\n","\n","    ## if prediction is positive get TTP\n","    if(classification == 1.0):\n","      final_TTP.append(round((time_to_result+30)/60, 2)) # 30 added because sample was taken 30s after actual reaction start\n","      print(f\"TTP: {time_to_result + 30}s \\t {round((time_to_result+30)/60, 2)} mins\")\n","    else:\n","      final_TTP.append(np.nan)\n","\n","    print(\"\")\n","    "],"metadata":{"id":"rptrj0arSjSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Accuracy: {accuracy(final_classifications)}\")\n","print(f\"Sensitivity/Recall: {sensitivity(final_classifications)}\")\n","print(f\"Specificity: {specificity(final_classifications)}\")\n","print(f\"Precision: {precision(final_classifications)}\")\n","print(f\"F1 Score: {f1(final_classifications)}\")"],"metadata":{"id":"6SilNigCLya5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Confusion Matrix"],"metadata":{"id":"MYCt8va8QtRd"}},{"cell_type":"code","source":["cm = confusion_matrix(true_labels, final_predictions, labels=[0, 1])\n","fig, ax = plt.subplots(1,1,figsize=(7,5))\n","heatmap = sns.heatmap(cm, annot=True, annot_kws={\"size\": 15}, linewidth=0.75, \n","            xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"], cbar=False, cmap='RdPu')\n","\n","heatmap.set_xticklabels(heatmap.get_xmajorticklabels(), fontsize = 15)\n","heatmap.set_yticklabels(heatmap.get_ymajorticklabels(), fontsize = 15)\n","\n","ax.set_ylabel('True Output', fontsize=15, labelpad=15)\n","ax.set_xlabel('Predicted Output', fontsize=15, labelpad=15)"],"metadata":{"id":"ORQMZoPIQ3Rm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Elbow Plot"],"metadata":{"id":"DgvFMAGtSbpy"}},{"cell_type":"code","source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"],"metadata":{"id":"i1xQt-8FxVXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## combine positive and negative sample dicts\n","all_samples = {}\n","all_samples.update(positives)\n","all_samples.update(negatives)\n","\n","## create dict of samples with true label\n","keys = list(all_samples.keys())\n","true_labels = list(np.concatenate((np.ones(len(positives)),np.zeros(len(negatives)))))\n","true_label_dict = dict(zip(keys, true_labels))"],"metadata":{"id":"-TubPsuIPuwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device(gpu):\n","  accuracies = []\n","  errors = []\n","\n","  ## iterate over different values of k \n","  for k in range(1,30,2):\n","    final_classifications = {}\n","\n","    ## use KNN to evaluate the prediction for each of the samples individually\n","    for key, value in all_samples.items():\n","      test_sample_name = key\n","      test_sample = value\n","\n","      ## get predictions for samples using KNN ensembles\n","      predictions = []\n","      for t in timestamps:\n","        train_data, train_labels = get_training_data_knn(positive_samples=positives, negative_samples=negatives, timestamp=t, test_samples=[test_sample_name])\n","        test_data = get_test_data_knn(test_sample, t)\n","        pred = KNN(k, test_data, train_data, train_labels, 'cosine')\n","        predictions.append(pred)\n","      \n","      time_index = get_time_index(timestamps, predictions) # get the value of the sample at which the sample needs to be indexed\n","      \n","      classification = Counter(predictions).most_common(1)[0][0] # final prediction\n","      final_classifications[key] = (classification, true_label_dict[key])\n","\n","    ## save accuracy for each K \n","    acc = accuracy(final_classifications)\n","    accuracies.append(acc)\n","    errors.append(1-acc)\n","    print(f\"K: {k} \\t Accuracy: {acc}\")\n"],"metadata":{"id":"WwBbkDraFWF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1,1, figsize=(10,5))\n","x = np.arange(1,30,2)\n","y = errors\n","axes.set_xlabel(\"K\")\n","axes.set_ylabel(\"Error Rate\")\n","axes.plot(x,y)"],"metadata":{"id":"P8t0E1pS9YNL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Confidence "],"metadata":{"id":"qXrQKuQm8ery"}},{"cell_type":"markdown","source":["#### Helper Functions"],"metadata":{"id":"oZRyLyYo7k36"}},{"cell_type":"code","source":["def get_rH_00(true_vals, classifier_outputs):\n","  \"\"\" Calculate ratio rH(y=0|y'=0) where y is the true label and y' is the predicted label\n","\n","  Parameters\n","  ----------\n","  true_vals : array\n","    True labels for each of the samples in the training data\n","  classifier_outputs : array\n","    Predicted output for each sample by a classifier at a given timestamp\n","\n","  Returns\n","  -------\n","  double\n","    rH(y=0|y'=0) where y is the true label and y' is the predicted label\n","  \"\"\"\n","  correct_pred_0 = 0\n","  total_pred_0 = 0\n","\n","  for idx, pred in enumerate(classifier_outputs):\n","    if(int(pred) == 0):\n","      total_pred_0 += 1\n","    if(int(pred) == 0 and int(true_vals[idx] == 0)):\n","      correct_pred_0 += 1\n","\n","  return correct_pred_0/total_pred_0\n","\n","\n","def get_rH_01(true_vals, classifier_outputs):\n","  \"\"\" Calculate ratio rH(y=0|y'=1) where y is the true label and y' is the predicted label\n","\n","  Parameters\n","  ----------\n","  true_vals : array\n","    True labels for each of the samples in the training data\n","  classifier_outputs : array\n","    Predicted output for each sample by a classifier at a given timestamp\n","\n","  Returns\n","  -------\n","  double\n","    rH(y=0|y'=1) where y is the true label and y' is the predicted label\n","  \"\"\"\n","  wrong_pred_1 = 0\n","  total_pred_1 = 0\n","\n","  for idx, pred in enumerate(classifier_outputs):\n","    if(int(pred) == 1):\n","      total_pred_1 += 1\n","    if(int(pred) == 1 and int(true_vals[idx] == 0)):\n","      wrong_pred_1 += 1\n","    \n","  return wrong_pred_1/total_pred_1\n","\n","def get_rH_10(true_vals, classifier_outputs):\n","  \"\"\" Calculate ratio rH(y=1|y'=0) where y is the true label and y' is the predicted label\n","\n","  Parameters\n","  ----------\n","  true_vals : array\n","    True labels for each of the samples in the training data\n","  classifier_outputs : array\n","    Predicted output for each sample by a classifier at a given timestamp\n","\n","  Returns\n","  -------\n","  double\n","    rH(y=1|y'=0) where y is the true label and y' is the predicted label\n","  \"\"\"\n","  wrong_pred_0 = 0\n","  total_pred_0 = 0\n","\n","  for idx, pred in enumerate(classifier_outputs):\n","    if(int(pred) == 0):\n","      total_pred_0 += 1\n","    if(int(pred) == 0 and int(true_vals[idx] == 1)):\n","      wrong_pred_0 += 1\n","  \n","  return wrong_pred_0/total_pred_0\n","\n","def get_rH_11(true_vals, classifier_outputs):\n","  \"\"\" Calculate ratio rH(y=1|y'=1) where y is the true label and y' is the predicted label\n","\n","  Parameters\n","  ----------\n","  true_vals : array\n","    True labels for each of the samples in the training data\n","  classifier_outputs : array\n","    Predicted output for each sample by a classifier at a given timestamp\n","\n","  Returns\n","  -------\n","  double\n","    rH(y=1|y'=1) where y is the true label and y' is the predicted label\n","  \"\"\"\n","  correct_pred_1 = 0\n","  total_pred_1 = 0\n","\n","  for idx, pred in enumerate(classifier_outputs):\n","    if(int(pred) == 1):\n","      total_pred_1 += 1\n","    if(int(pred) == 1 and int(true_vals[idx] == 1)):\n","      correct_pred_1 += 1\n","\n","  return correct_pred_1/total_pred_1"],"metadata":{"id":"6IQ6mN5uJ6z1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_confidence_multipliers(sample_predictions, true_labels):\n","\n","  \"\"\" Get the multpliers that are used to update the confidence score for each classifier\n","\n","  Parameters\n","  ----------\n","  sample_predictions : numpy.ndarray\n","    Array of predictions by each classifier for each sample in the training set \n","  true_labels : numpy.array\n","    True output labels for each sample in the training set\n","\n","  Returns\n","  -------\n","  multipliers_final : numpy.ndarray\n","    Array of multipliers for each classifer\n","  \"\"\"\n","\n","  sample_predictions = np.asarray(sample_predictions) # array of all predictions made by every classifer for all samples\n","\n","  #2d array of all possible multipliers for each classifier\n","  multipliers_final = []\n","\n","  # generate 4 multipliers for each classifier\n","  for classifier in range(len(sample_predictions[0])):\n","    classifier_output = sample_predictions[:, classifier]\n","\n","    rH_00 = get_rH_00(true_labels, classifier_output)\n","    rH_01 = get_rH_01(true_labels, classifier_output)\n","    rH_10 = get_rH_10(true_labels, classifier_output)\n","    rH_11 = get_rH_11(true_labels, classifier_output)\n","    multipliers_classifier = [rH_00, rH_01, rH_10, rH_11] \n","    \n","    # add multipliers to 2d array\n","    multipliers_final.append(multipliers_classifier)\n","\n","  return multipliers_final"],"metadata":{"id":"5r-wTh3nlga0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_confidence(preds, multipliers):\n","  \"\"\" Calculates the fused confidence for a given prediction\n","\n","  Parameters\n","  ----------\n","  preds : array(double)\n","    Array of all predictions upto and including the one that the confidence is being calculated for \n","  multipliers : numpy.ndarray\n","    Array of all multipliers for each classifier upto and including the one that made the given prediction being evaluated\n","\n","  Returns\n","  -------\n","  confidence : double\n","    Fused confidence for the prediction being evaluated\n","  \"\"\"\n","\n","  # initialise variable\n","  confidence = 1\n","\n","  # the prediction for which the confidence is being calculated -- predication at time t by classifier Ht (most recent prediction)\n","  pred_t = preds[-1]\n","\n","  for idx , pred in enumerate(preds):\n","    # prediction at time k made by classifier Hk\n","    pred_k = pred\n","\n","    # array of multipliers for Hk \n","    multiplier_k = multipliers[idx]\n","\n","    if(pred_t == 0 and pred_k == 0):\n","        confidence*=(1-multiplier_k[0])\n","    elif(pred_t == 0 and pred_k == 1):\n","        confidence*=(1-multiplier_k[1])\n","    elif(pred_t == 1 and pred_k == 0):\n","        confidence*=(1-multiplier_k[2])  \n","    elif(pred_t == 1 and pred_k == 1):\n","        confidence*=(1-multiplier_k[3])\n","        \n","        \n","\n","  confidence = 1 - confidence\n","\n","  return confidence\n","  "],"metadata":{"id":"8SmPp5iDzHQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_predictions_table(positives, negatives, timestamps):\n","\n","  \"\"\" Generate predictions for each sample in the training set one at a time\n","\n","  Parameters\n","  ----------\n","  positives : dict(pandas.DataFrame)\n","    Dictionary containing the data from the positive experiements \n","  negatives : dict(pandas.DataFrame)\n","    Dictionary containing the data from the negatives experiements \n","  timestamps : array(int)\n","    Array of timestamps at which predictions are made \n","  \n","  Returns\n","  -------\n","  sample_predictions : ndarray\n","    Array containing all predictions made by every classifier in the ensemble for experiments in the training set\n","  true_labels : array(int)\n","    Array of true class labels for every experiment in the training set\n","  \"\"\"\n","  sample_predictions = []\n","\n","  all_samples = {}\n","  all_samples.update(positives)\n","  all_samples.update(negatives)\n","\n","  true_labels = list(np.concatenate((np.ones(len(positives)),np.zeros(len(negatives)))))\n","\n","  ## use KNN to evaluate the prediction for each of the samples individually\n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    ## generate array of predictions\n","    predictions = []\n","    for t in timestamps:\n","      train_data, train_labels = get_training_data_knn(positive_samples=positives, negative_samples=negatives, timestamp=t, test_samples=[test_sample_name])\n","      test_data = get_test_data_knn(test_sample, t)\n","      pred = KNN(3, test_data, train_data, train_labels, 'cosine')\n","      predictions.append(pred)\n","      \n","    sample_predictions.append(predictions)\n","\n","  return sample_predictions, true_labels"],"metadata":{"id":"Qfe-e7-84_Zc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Learning best threshold"],"metadata":{"id":"KPuLuWoixCSR"}},{"cell_type":"code","source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"],"metadata":{"id":"HAfi2bU4phQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["number_of_samples = len(g1['Average Output'])\n","number_of_timestamps = 50\n","\n","timestep = int(number_of_samples/number_of_timestamps)\n","timestamps = [*range(timestep, number_of_samples+timestep, timestep)]"],"metadata":{"id":"ps-swVLHphQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## combine positive and negative sample dicts\n","all_samples = {}\n","all_samples.update(positives)\n","all_samples.update(negatives)\n","\n","## create dict of samples with true label\n","keys = list(all_samples.keys())\n","true_labels_array = list(np.concatenate((np.ones(len(positives)),np.zeros(len(negatives)))))\n","true_label_dict = dict(zip(keys, true_labels_array))"],"metadata":{"id":"r0eTLRQophQD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Generating candidates"],"metadata":{"id":"ErEA8XSGqh8a"}},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels = generate_predictions_table(positives, negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","  # sample index\n","  sample_idx = 0\n","\n","  # create set for all confidence values\n","  confidence_set = set()\n","  \n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    # get KNN predictions for the sample\n","    predictions = sample_predictions[sample_idx]\n","\n","    confidences = []\n","\n","    # for each prediction get the confidence and add to confidence array for the sample\n","    for i in range(len(predictions)):\n","      c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","      confidences.append(c)\n","    \n","    # update set with confidence values\n","    confidence_set = confidence_set.union(set(confidences))\n","    \n","    sample_idx += 1"],"metadata":{"id":"_m7lA1HIer-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["confidence_set = sorted(confidence_set)"],"metadata":{"id":"yVEuBQRb1BpE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold_candidates = set()\n","\n","# threshold candidates are the set of the mean of every pair of values in confidence set after sorting\n","for i in range(1,len(confidence_set)):\n","  mean = 0.5*(confidence_set[i] + confidence_set[i-1])\n","  threshold_candidates.add(mean) \n","\n","# sort candidates (only for ordering purposes)\n","threshold_candidates = sorted(threshold_candidates)"],"metadata":{"id":"rI2uBt6fxLlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(threshold_candidates)"],"metadata":{"id":"dTV9vJPW5zUg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Evaluating candidates"],"metadata":{"id":"872Hxw3fqlfv"}},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  # array to hold cost function value for each candidate\n","  cost_function_values = []\n","\n","  accs = []\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels = generate_predictions_table(positives, negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","  # alpha\n","  alpha = 0.85\n","\n","  # evaluate every candidate\n","  for th in threshold_candidates:\n","\n","    print(f\"Candidate: {th} \")\n","\n","    # array to hold earliness values for the samples \n","    earliness = []  \n","\n","    # dict to hold predictions vs true values for the samples  \n","    final_classifications = {}\n","\n","    # sample index\n","    sample_idx = 0\n","\n","    for key, value in all_samples.items():\n","      test_sample_name = key\n","      test_sample = value\n","\n","      # get KNN predictions for the sample\n","      predictions = sample_predictions[sample_idx]\n","\n","      for i in range(len(predictions)):\n","        \n","        # get the confidence for that prediction \n","        c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","\n","        if(c >= th): # check if confidence is at or above confidence threshold\n","\n","          time_index = timestamps[i] # get the value of the sample number at which the sample needs to be indexed\n","          time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time according to the experiment at which the result is obtained\n","          \n","          # predicted class for the sample is given by the prediction which led to the given confidence value\n","          pred = predictions[i]\n","\n","          # update final outcomes dict\n","          final_classifications[test_sample_name] = (pred, true_label_dict[test_sample_name])\n","\n","          # add to earliness array\n","          earliness.append(time_index/timestamps[-1])\n","\n","          break\n","\n","        if(i == len(predictions) - 1): # if threshold is not met ever -- result is inconclusive\n","          final_classifications[test_sample_name] = (None, true_label_dict[test_sample_name])\n","      \n","      sample_idx += 1\n","\n","    # get avg accuracy and avg earliness for this threshold\n","    if(len(final_classifications) > 0):\n","      avg_accuracy = accuracy(final_classifications)\n","      avg_earliness = sum(earliness)/len(earliness)\n","      accs.append(avg_accuracy)\n","      \n","      # compute value of cost function and add to array \n","      cf_score = alpha*(1-avg_accuracy) + (1-alpha)*avg_earliness\n","      cost_function_values.append(cf_score)\n","      print(f\"Score: {cf_score}\")\n","      print(\"\")\n"],"metadata":{"id":"gKc1S0R6ebff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# aim is to minimise cost function -- find index in array where this is the case\n","lowest_cf_score = np.min(np.array(cost_function_values))\n","index_best_th = np.argmin(np.array(cost_function_values))"],"metadata":{"id":"P2IsLZXQ77oZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lowest_cf_score"],"metadata":{"id":"4iZ3ba0jhY0I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_best_th"],"metadata":{"id":"dBS0v3GAh77m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_th = list(threshold_candidates)[index_best_th]\n","best_th"],"metadata":{"id":"nmy0bv8N8YvW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Testing with best threshold"],"metadata":{"id":"ypFTpLSAir09"}},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  final_classifications = {}\n","  final_predictions = []\n","  final_TTP = []\n","  earliness = []\n","  prediction_correctness = []\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels = generate_predictions_table(positives, negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","  # sample index\n","  sample_idx = 0\n","\n","  # count inconclusive results\n","  inconc_count = 0\n","  \n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    print(f\"Sample {test_sample_name}\")  \n","    predictions = sample_predictions[sample_idx]\n","\n","    # get confidence for each prediction\n","    for i in range(len(predictions)):\n","\n","      c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","\n","      # check if confidence meets threshold\n","      if(c >= best_th): # best confidence threshold from cost function\n","        time_index = timestamps[i] # get the value of the sample number at which the sample needs to be indexed\n","        time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time acorrding the experiment at which result is obtained\n","\n","        # final prediction\n","        pred = predictions[i]\n","        \n","        # update arrays with final output\n","        final_classifications[test_sample_name] = (pred, true_label_dict[test_sample_name])\n","        final_predictions.append(pred)\n","        prediction_correctness.append(\"Yes\" if pred == true_label_dict[key] else \"No\")\n","        print(f\"Predicted Label: {pred} \\t True Label: {true_label_dict[test_sample_name]} \\t Correct?: {pred == true_label_dict[test_sample_name]}\")\n","\n","        earliness.append(time_index/timestamps[-1])\n","\n","        ## if final output is positive then get TTPs\n","        if(pred == 1.0):\n","          final_TTP.append(round((time_to_result+30)/60, 2)) # 30 added because sample was taken 30s after actual reaction start\n","          print(f\"TTP: {time_to_result + 30}s \\t {round((time_to_result+30)/60, 2)} mins\")\n","        else:\n","          final_TTP.append(np.nan)\n","\n","\n","        break\n","\n","      # if confidence is never met then then result is inconclusive\n","      if(i == len(predictions)-1):\n","        final_classifications[test_sample_name] = (None, true_label_dict[test_sample_name])\n","        final_predictions.append(None)\n","        print(\"Inconclusive\")\n","        inconc_count += 1\n","    \n","    sample_idx += 1\n","    print(\"\")\n","\n","  print(f\"Accuracy: {accuracy(final_classifications)}\")\n","  print(f\"Sensitivity/Recall: {sensitivity(final_classifications)}\")\n","  print(f\"Specificity: {specificity(final_classifications)}\")\n","  print(f\"Precision: {precision(final_classifications)}\")\n","  print(f\"F1 Score: {f1(final_classifications)}\")\n","  print(f\"Average Earliness: {sum(earliness)/len(earliness)}\")\n","  print(f\"Total Inconclusive: {inconc_count}/{len(sample_predictions)}\")"],"metadata":{"id":"LfPwqWWHiqnY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Confusion Matrix"],"metadata":{"id":"kvp33iLnnBBA"}},{"cell_type":"code","source":["cm = confusion_matrix(true_labels, final_predictions, labels=[0, 1])\n","fig, ax = plt.subplots(1,1,figsize=(7,5))\n","heatmap = sns.heatmap(cm, annot=True, annot_kws={\"size\": 15}, linewidth=0.75, \n","            xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"], cbar=False, cmap='RdPu')\n","\n","heatmap.set_xticklabels(heatmap.get_xmajorticklabels(), fontsize = 15)\n","heatmap.set_yticklabels(heatmap.get_ymajorticklabels(), fontsize = 15)\n","\n","ax.set_ylabel('True Output', fontsize=15, labelpad=15)\n","ax.set_xlabel('Predicted Output', fontsize=15, labelpad=15)"],"metadata":{"id":"8iulHExyn1e3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Testing with different alpha values"],"metadata":{"id":"w43_TkUVitvi"}},{"cell_type":"code","source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"],"metadata":{"id":"pJXWD05RjB-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["number_of_samples = len(g1['Average Output'])\n","number_of_timestamps = 50\n","\n","timestep = int(number_of_samples/number_of_timestamps)\n","timestamps = [*range(timestep, number_of_samples+timestep, timestep)]"],"metadata":{"id":"NSkTWcddjB-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(timestamps)"],"metadata":{"id":"KWOXfxRKjB-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## combine positive and negative sample dicts\n","all_samples = {}\n","all_samples.update(positives)\n","all_samples.update(negatives)\n","\n","## create dict of samples with true labels\n","keys = list(all_samples.keys())\n","true_labels_array = list(np.concatenate((np.ones(len(positives)),np.zeros(len(negatives)))))\n","true_label_dict = dict(zip(keys, true_labels_array))"],"metadata":{"id":"NXrd_74JjB-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels = generate_predictions_table(positives, negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","  # sample index\n","  sample_idx = 0\n","\n","  # create set for all confidence values\n","  confidence_set = set()\n","  \n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    # get KNN predictions for the sample\n","    predictions = sample_predictions[sample_idx]\n","\n","    confidences = []\n","\n","    # for each prediction get the confidence and add to confidence array for the sample\n","    for i in range(len(predictions)):\n","      c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","      confidences.append(c)\n","    \n","    # update set with confidence values\n","    confidence_set = confidence_set.union(set(confidences))\n","    \n","    sample_idx += 1"],"metadata":{"id":"wcE9LintjB-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["confidence_set = sorted(confidence_set)"],"metadata":{"id":"qb9FWSPgjB-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold_candidates = set()\n","\n","# threshold candidates are mad of the mean of every pair of values in confidence set after sorting\n","for i in range(1,len(confidence_set)):\n","  mean = 0.5*(confidence_set[i] + confidence_set[i-1])\n","  threshold_candidates.add(mean) \n","\n","# sort candidates (only for ordering purposes)\n","threshold_candidates = sorted(threshold_candidates)"],"metadata":{"id":"bM7ILJSwjB-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(threshold_candidates)"],"metadata":{"id":"36WDfG-TjB-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  acc = []\n","  ear = []\n","\n","######################################################## Optimal Threshold Calculation #############################################################\n","  # iterate over possible alpha values\n","  for i in range(0,100,5):\n","\n","    # alpha\n","    alpha = i/100\n","\n","    print(f\"Alpha: {alpha}\")\n","\n","    # array to hold cost function value for each candidate\n","    cost_function_values = []\n","\n","    # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","    sample_predictions, true_labels = generate_predictions_table(positives, negatives, timestamps)\n","\n","    # create multipliers for every classifier\n","    multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","    # evaluate every threshold candidate\n","    for th in threshold_candidates:\n","\n","      # array to hold earliness values for the samples \n","      earliness = []  \n","\n","      # dict to hold predictions vs true values for the samples  \n","      final_classifications = {}\n","\n","      # sample index\n","      sample_idx = 0\n","\n","      for key, value in all_samples.items():\n","        test_sample_name = key\n","        test_sample = value\n","  \n","        # get KNN predicition for the sample\n","        predictions = sample_predictions[sample_idx]\n","\n","        for i in range(len(predictions)):\n","\n","          # get the confidence for that prediction \n","          c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","\n","          if(c >= th): # check if confidence is above confidence threshold\n","\n","            time_index = timestamps[i] # get the value of the sample number at which the sample needs to be indexed\n","            time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time acorrding the experiment at which result is obtained\n","\n","            # predicted class for the sample is given by the prediction which led to the gien confidence value\n","            pred = predictions[i]\n","\n","            # update final outcomes dict\n","            final_classifications[test_sample_name] = (pred, true_label_dict[test_sample_name])\n","\n","            # add to earliness array\n","            earliness.append(time_index/timestamps[-1])\n","\n","            break\n","          \n","          if(i == len(predictions) - 1): # if threshold is not met ever -- result is inconclusive\n","            final_classifications[test_sample_name] = (None, true_label_dict[test_sample_name])\n","\n","        sample_idx += 1\n","\n","      # get avg accuracy and avg earliness for this threshold\n","      if(len(final_classifications) > 0):\n","        avg_accuracy = accuracy(final_classifications)\n","        avg_earliness = sum(earliness)/len(earliness)\n","\n","        # compute value of cost function and add to array \n","        cf_score = alpha*(1-avg_accuracy) + (1-alpha)*avg_earliness\n","        cost_function_values.append(cf_score)\n","\n","    index_best_th = np.argmin(np.array(cost_function_values))    \n","    best_th = list(threshold_candidates)[index_best_th]\n","\n","###################################################### Testing with optimal theshold ############################################################\n","\n","    final_classifications = {}\n","    earliness = []\n","\n","    # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","    sample_predictions, true_labels = generate_predictions_table(positives, negatives, timestamps)\n","\n","    # create multipliers for every classifier\n","    multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","    # sample index\n","    sample_idx = 0\n","\n","    # count inconclusive results\n","    inconc_count = 0\n","    \n","    ## use KNN to evaluate the prediction for each of the samples individually\n","    for key, value in all_samples.items():\n","      test_sample_name = key\n","      test_sample = value\n","\n","      predictions = sample_predictions[sample_idx]\n","\n","      for i in range(len(predictions)):\n","        c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","      \n","        if(c >= best_th): # best confidence threshold from cost function\n","          time_index = timestamps[i] # get the value of the sample number at which the sample needs to be indexed\n","          time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time acorrding the experiment at which result is obtained\n","\n","          # final prediction \n","          pred = predictions[i]\n","\n","          # update arrays with final outcome and time to result\n","          final_classifications[test_sample_name] = (pred, true_label_dict[test_sample_name])\n","          earliness.append(time_index/timestamps[-1])\n","          break\n","\n","        # if threshold is not met then result is inconclusive\n","        if(i == len(predictions)-1):\n","          final_classifications[test_sample_name] = (None, true_label_dict[test_sample_name])\n","          inconc_count += 1\n","      \n","      sample_idx += 1\n","\n","    print(f\"Avg Accuracy: {accuracy(final_classifications)}\")\n","    print(f\"Avg Earliness: {sum(earliness)/len(earliness)}\")\n","    print(\"\")\n","    acc.append(accuracy(final_classifications))\n","    ear.append(sum(earliness)/len(earliness))"],"metadata":{"id":"V6Fhz4hsiui6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot earlines vs accuracy graph\n","\n","fig, axes = plt.subplots(1,1, figsize=(10,5))\n","x = acc\n","y = ear\n","axes.set_xlabel(\"Accuracy\")\n","axes.set_ylabel(\"Earliness\")\n","axes.plot(x,y, '-o')"],"metadata":{"id":"R0a7Bbrabp6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dummy Data"],"metadata":{"id":"50ehoM0ot2lV"}},{"cell_type":"markdown","source":["#### Experimental Data (only use after pre-processing done)"],"metadata":{"id":"Bpkjt_A5t7dK"}},{"cell_type":"code","source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"],"metadata":{"id":"1Ll5v4F5uFui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## combine positive and negative sample dicts\n","all_samples = {}\n","all_samples.update(positives)\n","all_samples.update(negatives)\n","\n","## create dict of samples with true labels\n","keys = list(all_samples.keys())\n","true_labels = list(np.concatenate((np.ones(len(positives)),np.zeros(len(negatives)))))\n","true_label_dict = dict(zip(keys, true_labels))"],"metadata":{"id":"Yi-4wkjiuFuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot average drift for each experiement calculated during pre-processing stage\n","\n","fig, axes = plt.subplots(1,1, figsize=(10,5))\n","for key, sample in all_samples.items():\n","  axes.plot(sample.index, np.array(sample['Average Drift']))"],"metadata":{"id":"OSPnRJTVAxMJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Get drift params from average drift curves"],"metadata":{"id":"VBsUVoXg8SFW"}},{"cell_type":"code","source":["fig, axes = plt.subplots(1,1, figsize=(10,5))\n","opt = np.zeros((2, len(keys)))\n","\n","for idx, sample in enumerate(all_samples.values()):\n","  try:\n","    # extrapolate to fit the drift curves to estimate params\n","    opt[:2, idx], _ = curve_fit(decaying_exp, sample.index, np.array(sample['Average Drift']), p0=[-10, 0.1])\n","\n","    # plot generated curves\n","    axes.plot(sample.index, decaying_exp(sample.index, *opt[:2,idx]))\n","  except:\n","    opt[:, idx] = np.nan\n","    continue"],"metadata":{"id":"1K_YM_2RuInW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Plotting drift params and corresponding outcomes"],"metadata":{"id":"JcGL_puI9tra"}},{"cell_type":"code","source":["# plot drift model params for the positive and negative reactions\n","\n","fig, axes = plt.subplots(1,1, figsize=(10,5))\n","axes.set_xlabel(\"a\")\n","axes.set_ylabel(\"b\")\n","axes.set_title(\"Drift model parameters for positives and negatives\")\n","\n","no_pos_samples = len(positives) \n","axes.scatter(opt[0, :no_pos_samples], opt[1, :no_pos_samples], color='red', alpha=0.5, label=\"Positives\")\n","axes.scatter(opt[0, no_pos_samples:], opt[1, no_pos_samples:], color='blue', alpha=0.5, label=\"Negatives\")\n","\n","plt.legend(fontsize=12)"],"metadata":{"id":"yBCHF-2SDrvZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Mean and standard deviation of drift params (calculated from experimenal data)"],"metadata":{"id":"xTz9Msjf8Zlm"}},{"cell_type":"code","source":["mean_a = np.nanmean(opt[0, :])\n","mean_b = np.nanmean(opt[1, :])\n","\n","std_a = np.nanstd(opt[0, :])\n","std_b = np.nanstd(opt[1, :])"],"metadata":{"id":"U9Yj45DExVDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Mean a: {mean_a}\")\n","print(f\"Mean b: {mean_b}\")\n","print(f\"Std a: {std_a}\")\n","print(f\"Std b: {std_b}\")"],"metadata":{"id":"UoFAt9sV4o03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dummy Negatives"],"metadata":{"id":"s7rcrMLS8rVL"}},{"cell_type":"code","source":["# mean and std for the drift params used for data generation \n","\n","mean_a_dummy_data = mean_a\n","std_a_dummy_data = 10\n","\n","mean_b_dummy_data = 0.0025\n","std_b_dummy_data = 0.001"],"metadata":{"id":"P9qV8UdAO07n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_range = [*range(0,1200,3)]\n","\n","# fig, ax = plt.subplots(1,1, figsize=(10,5))\n","\n","dummy_negatives = {}\n","\n","for i in range(100):\n","\n","  # generate params for drift curve\n","  params = np.zeros((2,1))\n","  params[0, 0] = np.random.normal(mean_a_dummy_data, std_a_dummy_data)\n","  params[1, 0] = np.random.normal(mean_b_dummy_data, std_b_dummy_data)\n","  \n","  # create curve \n","  y_vals = decaying_exp(x_range, *params)\n","  y_vals -= 9 ## must be done to ensure both positives and neagtives start at the same y value\n","  \n","  # create df for negative sample\n","  df = pd.DataFrame(y_vals, index=x_range, columns=['Average Output'])\n","  dummy_negatives[f\"Train_Neg{i}\"] = df\n","\n","  # plotting graphs\n","  # ax.set_xlabel(\"Time (s)\")\n","  # ax.set_ylabel(\"Average Output (mV)\")\n","  # ax.set_title(\"Dummy Negatives Train\")\n","  # ax.plot(x_range, y_vals)"],"metadata":{"id":"vyiR4MvizsVX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dummy Negatives Test"],"metadata":{"id":"Qrg1gA9Gv7R-"}},{"cell_type":"code","source":["x_range = [*range(0,1200,3)]\n","\n","# fig, ax = plt.subplots(1,1, figsize=(10,5))\n","\n","dummy_negatives_test = {}\n","\n","for i in range(100):\n","\n","  # generate params for drift curve\n","  params = np.zeros((2,1))\n","  params[0, 0] = np.random.normal(mean_a_dummy_data, std_a_dummy_data)\n","  params[1, 0] = np.random.normal(mean_b_dummy_data, std_b_dummy_data)\n","  \n","  # create curve\n","  y_vals = decaying_exp(x_range, *params)\n","  y_vals -= 9 ## must be done to ensure both positives and neagtives start at the same y value\n","  \n","  # create df for negative sample\n","  df = pd.DataFrame(y_vals, index=x_range, columns=['Average Output'])\n","  dummy_negatives_test[f\"Test_Neg{i}\"] = df\n","\n","  # plotting graphs\n","  # ax.set_xlabel(\"Time (s)\")\n","  # ax.set_ylabel(\"Average Output (mV)\")\n","  # ax.set_title(\"Dummy Negatives Test\")\n","  # ax.plot(x_range, y_vals)"],"metadata":{"id":"6HIoSrJZv54T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dummy Positives Model"],"metadata":{"id":"BRf7xD5U8vab"}},{"cell_type":"code","source":["def log_sigmoid(x, ttp):\n","\n","  \"\"\" Generate the log of a sigmoid to model the positive amplification curve\n","\n","  Parameters\n","  ----------\n","  x : array\n","    Times at which the curve is to be generated for \n","  ttp : int\n","    The translation of the curve in the time axis\n","\n","  Returns\n","  -------\n","  sig : array\n","    The values generated when the sigmoid function is applied to the x values\n","  log_sig : array \n","    The values generated when aplying the sigmoid then taking log \n","  \"\"\"  \n","  \n","  sig = []\n","  for val in x:\n","    s_val = 10e-10 + (10e-8 - 10e-10)/( 1+math.exp( -0.02 * (val-ttp) ) ) ## 10e-10 added to start at 10^-9 and also subtracted from t_inf because the graph should saturate at 10^-8\n","    sig.append(s_val)\n","\n","  log_sig = np.log10(np.array(sig))\n","\n","  return sig, log_sig"],"metadata":{"id":"Tk3HARgp7m4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dummy Positives"],"metadata":{"id":"p8pEEz4o3pVY"}},{"cell_type":"code","source":["# mean and std for the ttp when being sampled from a normal distribution\n","\n","mean_ttp = 900\n","std_ttp = 100"],"metadata":{"id":"95dg5fSEhUzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_range = [*range(0,1200,3)]\n","\n","# fig, ax = plt.subplots(3,1, figsize=(10,15))\n","\n","dummy_positives = {}\n","\n","for i in range(100):\n","  params = np.zeros((2,1))\n","  params[0, 0] = np.random.normal(mean_a_dummy_data, std_a_dummy_data) # a for drift\n","  params[1, 0] = np.random.normal(mean_b_dummy_data, std_b_dummy_data) # b for drift\n","\n","  ## create drfit\n","  y_drift_vals = decaying_exp(x_range, *params)\n","\n","  ## create sigmoid\n","  ttp = np.random.normal(mean_ttp, std_ttp) # ttp for drift mean of 15 mins with std of 5/3 mins (99% of ttps within 10-20 mins which is 3 std away)\n","  y_sig_vals, y_log_sig_vals = log_sigmoid(x_range, ttp)\n","\n","  ## add sigmoid to drift\n","  pos_vals = y_drift_vals + y_log_sig_vals\n","\n","  ## convert to df as positive sample\n","  df = pd.DataFrame(pos_vals, index=x_range, columns=['Average Output'])\n","  dummy_positives[f\"Train_Pos{i}\"] = df\n","\n","  # plotting graphs\n","  # ax[0].set_xlabel(\"Time (s)\")\n","  # ax[0].set_title(\"Sigmoid\")\n","\n","  # ax[1].set_xlabel(\"Time (s)\")\n","  # ax[1].set_title(\"Log10 Sigmoid\")\n","\n","  # ax[2].set_xlabel(\"Time (s)\")\n","  # ax[2].set_ylabel(\"Average Output (mV)\")\n","  # ax[2].set_title(\"Dummy Positives Train\")\n","\n","  # ax[0].plot(x_range, y_sig_vals)\n","  # ax[1].plot(x_range, y_log_sig_vals)\n","  # ax[2].plot(x_range, pos_vals)\n"],"metadata":{"id":"Jbe8z8iTdOT4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dummy Positives Test"],"metadata":{"id":"lO5IRnn1wAuB"}},{"cell_type":"code","source":["x_range = [*range(0,1200,3)]\n","\n","# fig, ax = plt.subplots(3,1, figsize=(10,15))\n","\n","dummy_positives_test = {}\n","\n","for i in range(100):\n","  params = np.zeros((2,1))\n","  params[0, 0] = np.random.normal(mean_a_dummy_data, std_a_dummy_data) # a for drift\n","  params[1, 0] = np.random.normal(mean_b_dummy_data, std_b_dummy_data) # b for drift\n","\n","  ## create drfit\n","  y_drift_vals = decaying_exp(x_range, *params)\n","\n","  ## create sigmoid\n","  ttp = np.random.normal(mean_ttp, std_ttp) # ttp for drift mean of 15 mins with std of 5/3 mins (99% of ttps within 10-20 mins which is 3 std away)\n","  y_sig_vals, y_log_sig_vals = log_sigmoid(x_range, ttp)\n","\n","  ## add sigmoid to drift\n","  pos_vals = y_drift_vals + y_log_sig_vals\n","\n","  ## convert to df as positive sample\n","  df = pd.DataFrame(pos_vals, index=x_range, columns=['Average Output'])\n","  dummy_positives_test[f\"Test_Pos{i}\"] = df\n","\n","  # plotting graphs\n","  # ax[0].set_xlabel(\"Time (s)\")\n","  # ax[0].set_title(\"Sigmoid\")\n","\n","  # ax[1].set_xlabel(\"Time (s)\")\n","  # ax[1].set_title(\"Log10 Sigmoid\")\n","\n","  # ax[2].set_xlabel(\"Time (s)\")\n","  # ax[2].set_ylabel(\"Average Output (mV)\")\n","  # ax[2].set_title(\"Dummy Positives Test\")\n","\n","  # ax[0].plot(x_range, y_sig_vals)\n","  # ax[1].plot(x_range, y_log_sig_vals)\n","  # ax[2].plot(x_range, pos_vals)\n"],"metadata":{"id":"HDMvy2sxwCkO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Load Sythetic Data (if needed)"],"metadata":{"id":"Os0rsgqGdRfr"}},{"cell_type":"code","source":["# pos_train_file = '/SyntheticData/Iteration6/PosTrain.pkl'\n","# pos_test_file = '/SyntheticData/Iteration6/PosTest.pkl'\n","\n","# neg_train_file = '/SyntheticData/Iteration6/NegTrain.pkl'\n","# neg_test_file = '/SyntheticData/Iteration6/NegTest.pkl'\n","\n","# dummy_positives = load_data(pos_train_file)\n","# dummy_positives_test = load_data(pos_test_file)\n","# dummy_negatives = load_data(neg_train_file)\n","# dummy_negatives_test = load_data(neg_test_file)"],"metadata":{"id":"WDPEETRycbHN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Learning Best Threshold"],"metadata":{"id":"GvyCtQ1YiDI3"}},{"cell_type":"code","source":["x_range = [*range(0,1200,3)]\n","\n","number_of_samples = len(x_range)\n","number_of_timestamps = 50\n","\n","timestep = int(number_of_samples/number_of_timestamps)\n","timestamps = [*range(timestep, number_of_samples+timestep, timestep)]"],"metadata":{"id":"QEcTtJnpiGHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## combine positive and negative sample dicts for the training data\n","all_samples = {}\n","all_samples.update(dummy_positives)\n","all_samples.update(dummy_negatives)\n","\n","## create dict of samples with true labels for the training data\n","keys = list(all_samples.keys())\n","true_labels_array = list(np.concatenate((np.ones(len(dummy_positives)),np.zeros(len(dummy_negatives)))))\n","true_label_dict = dict(zip(keys, true_labels_array))"],"metadata":{"id":"VHjUP_OjiRPo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Generating candidates"],"metadata":{"id":"z5Mpi1gWjhMH"}},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels = generate_predictions_table(dummy_positives, dummy_negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","  # sample index\n","  sample_idx = 0\n","\n","  # create set for all confidence values\n","  confidence_set = set()\n","  \n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    # get KNN predictions for the sample\n","    predictions = sample_predictions[sample_idx]\n","\n","    confidences = []\n","\n","    # for each prediction get the confidence and add to confidence array for the sample\n","    for i in range(len(predictions)):\n","      c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","      confidences.append(c)\n","\n","    # update set with confidence values\n","    confidence_set = confidence_set.union(set(confidences))\n","    \n","    sample_idx += 1"],"metadata":{"id":"_IFcm2WkjhMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["confidence_set = sorted(confidence_set)"],"metadata":{"id":"svxb8CsbjhMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold_candidates = set()\n","\n","# threshold candidates are the set of the mean of every pair of values in confidence set after sorting\n","for i in range(1,len(confidence_set)):\n","  mean = 0.5*(confidence_set[i] + confidence_set[i-1])\n","  threshold_candidates.add(mean) \n","\n","# sort candidates (only for ordering purposes)\n","threshold_candidates = sorted(threshold_candidates)"],"metadata":{"id":"_MbgZRvljhMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(threshold_candidates)"],"metadata":{"id":"MpoMo8hI1Oke"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Evaluating candidates"],"metadata":{"id":"6XU2POFskVrn"}},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  # array to hold cost function value for each candidate\n","  cost_function_values = []\n","\n","  accs = []\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels = generate_predictions_table(dummy_positives, dummy_negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels)\n","\n","  # alpha\n","  alpha = 0.85\n","\n","  # evaluate every threshold candidate\n","  for th in threshold_candidates:\n","\n","    print(f\"Candidate: {th} \")\n","\n","    # array to hold earliness values for the samples \n","    earliness = []  \n","\n","    # dict to hold predictions vs true values for the samples  \n","    final_classifications = {}\n","\n","    # sample index\n","    sample_idx = 0\n","\n","    for key, value in all_samples.items():\n","      test_sample_name = key\n","      test_sample = value\n","\n","      # get KNN predictions for the sample\n","      predictions = sample_predictions[sample_idx]\n","\n","      for i in range(len(predictions)):\n","        \n","        # get the confidence for that prediction \n","        c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","\n","        if(c >= th): # check if confidence is at or above confidence threshold\n","\n","          time_index = timestamps[i] # get the value of the sample number at which the sample needs to be indexed\n","          time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time according to the experiment at which the result is obtained\n","          \n","          # predicted class for the sample is given by the prediction which led to the given confidence value\n","          pred = predictions[i]\n","\n","          # update final outcomes dict\n","          final_classifications[test_sample_name] = (pred, true_label_dict[test_sample_name])\n","\n","          # add to earliness array\n","          earliness.append(time_index/timestamps[-1])\n","\n","          break\n","          \n","        if(i == len(predictions) - 1): # if threshold is not met ever -- result is inconclusive\n","          final_classifications[test_sample_name] = (None, true_label_dict[test_sample_name])\n","\n","      sample_idx += 1\n","\n","    # get avg accuracy and avg earliness for this threshold\n","    if(len(final_classifications) > 0):\n","      avg_accuracy = accuracy(final_classifications)\n","      avg_earliness = sum(earliness)/len(earliness)\n","      accs.append(avg_accuracy)\n","      \n","      # compute value of cost function and add to array \n","      cf_score = alpha*(1-avg_accuracy) + (1-alpha)*avg_earliness\n","      cost_function_values.append(cf_score)\n","      print(f\"Score: {cf_score}\")\n","      print(\"\")\n"],"metadata":{"id":"qBswRqA9kVrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# aim is to minimise cost function -- find index in array where this is the case\n","lowest_cf_score = np.min(np.array(cost_function_values))\n","index_best_th = np.argmin(np.array(cost_function_values))"],"metadata":{"id":"z_7kctZdkVrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lowest_cf_score"],"metadata":{"id":"5kQpY7zTkVro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_best_th"],"metadata":{"id":"1b-lyMIzkVro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_th = list(threshold_candidates)[index_best_th]\n","best_th"],"metadata":{"id":"UO8zfKGvkVro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Testing with best threshold (on test set)"],"metadata":{"id":"DKbProYAmi_D"}},{"cell_type":"code","source":["## combine positive and negative sample dicts for the test data\n","all_samples_test = {}\n","all_samples_test.update(dummy_positives_test)\n","all_samples_test.update(dummy_negatives_test)\n","\n","## create dict of samples with true label for the test data\n","keys_test = list(all_samples_test.keys())\n","true_labels_test_array = list(np.concatenate((np.ones(len(dummy_positives_test)),np.zeros(len(dummy_negatives_test)))))\n","true_label_test_dict = dict(zip(keys_test, true_labels_test_array))"],"metadata":{"id":"fPs8NbYXwNGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device(gpu):\n","\n","  final_classifications = {}\n","  ttps = []\n","  earliness = []\n","\n","  # create nN predictions using each sample in the training dataset as the test sample (n = no of exps in train set, N = no of classifiers)\n","  sample_predictions, true_labels_train = generate_predictions_table(dummy_positives, dummy_negatives, timestamps)\n","\n","  # create multipliers for every classifier\n","  multipliers_2d = get_confidence_multipliers(sample_predictions, true_labels_train)\n","\n","  # sample index\n","  sample_idx = 0\n","\n","  # count inconclusive results\n","  inconc_count = 0\n","  \n","  # evaluate every experiment in the test set\n","  for key, value in all_samples_test.items():\n","    test_sample_name = key\n","    test_sample = value\n","\n","    print(f\"Sample {test_sample_name}\")  \n","    predictions = []\n","\n","    # get the predictions for each timestamp\n","    for i, t in enumerate(timestamps):\n","\n","      train_data, train_labels = get_training_data_knn(positive_samples=dummy_positives, negative_samples=dummy_negatives, timestamp=t, test_samples=[test_sample_name])\n","      test_data = get_test_data_knn(test_sample, t)\n","      pred = KNN(3, test_data, train_data, train_labels, 'cosine')\n","      predictions.append(pred)\n","\n","      # get confidence in the current prediction\n","      c = get_confidence(predictions[:i+1], multipliers_2d[:i+1]) # i+1 needed because slicing does not include last index \n","\n","      # check if confidence meets threshold\n","      if(c >= best_th): # best confidence threshold from cost function\n","      \n","        time_index = timestamps[i] # get the value of the sample number at which the sample needs to be indexed\n","        time_to_result = test_sample.index[time_index-1] - test_sample.index[0] # get actual time acorrding the experiment at which result is obtained\n","\n","        # final prediction\n","        pred = predictions[i]\n","        \n","        # update arrays with final outcomes and time to result\n","        final_classifications[test_sample_name] = (pred, true_label_test_dict[test_sample_name])\n","\n","        print(f\"Predicted Label: {pred} \\t True Label: {true_label_test_dict[test_sample_name]} \\t Correct?: {pred == true_label_test_dict[test_sample_name]}\")\n","\n","        earliness.append(time_index/timestamps[-1])\n","\n","        # if final output is positive get ttp\n","        if(pred == 1.0):\n","          print(f\"TTP: {time_to_result}s \\t {round((time_to_result)/60, 2)} mins\")\n","\n","        break\n","\n","      # if threshold is never met then result is inconclusive\n","      if(i == len(timestamps)-1):\n","        final_classifications[test_sample_name] = (None, true_label_test_dict[test_sample_name])\n","        print(\"Inconclusive\")\n","        inconc_count += 1\n","    \n","    sample_idx += 1\n","    print(\"\")\n","\n","  print(f\"Accuracy: {accuracy(final_classifications)}\")\n","  print(f\"Sensitivity/Recall: {sensitivity(final_classifications)}\")\n","  print(f\"Specificity: {specificity(final_classifications)}\")\n","  print(f\"Precision: {precision(final_classifications)}\")\n","  print(f\"F1 Score: {f1(final_classifications)}\")\n","  print(f\"Average Earliness: {sum(earliness)/len(earliness)}\")\n","  print(f\"Total Inconclusive: {inconc_count}/{sample_idx}\")"],"metadata":{"id":"7pSh9BTjyCD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the positive samples that have been wrongly classified as negative\n","\n","# x_range = [*range(0,1200,3)]\n","\n","# fig, ax = plt.subplots(1,1, figsize=(10,5))\n","\n","# ax.set_xlabel(\"Time (s)\")\n","# ax.set_ylabel(\"Average Output (mV)\")\n","# ax.set_title(\"Mis-classified Positives\")\n","\n","# for k,v in final_classifications.items():\n","#   if(v[0] != v[1] and v[1] == 1.0):\n","#     df = dummy_positives_test[k]\n","#     y_vals = df['Average Output']\n","#     ax.plot(x_range, y_vals)"],"metadata":{"id":"jZTX7JOsaM4j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Save Sythetic Data (for replication of results)"],"metadata":{"id":"HjXJ8JYbDTpO"}},{"cell_type":"code","source":["# pos_train_file = '/SyntheticData/Iteration10/PosTrain.pkl'\n","# pos_test_file = '/SyntheticData/Iteration10/PosTest.pkl'\n","\n","# neg_train_file = '/SyntheticData/Iteration10/NegTrain.pkl'\n","# neg_test_file = '/SyntheticData/Iteration10/NegTest.pkl'\n","\n","# save_data(pos_train_file, dummy_positives)\n","# save_data(pos_test_file, dummy_positives_test)\n","# save_data(neg_train_file, dummy_negatives)\n","# save_data(neg_test_file, dummy_negatives_test)"],"metadata":{"id":"dIvE1L_TDXlj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"simepEdKIiL0"},"source":["### Github Commands"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QqMeViIQjzhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"YdlGDV3AzZ1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itbAqo9qGukN"},"outputs":[],"source":["username = \"adityag16\"\n","git_token = \"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\"\n","repository = \"/content/drive/MyDrive/Final-Year-Project\"\n","%cd {repository}\n","!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNInxPqdG7nx"},"outputs":[],"source":["!git add 'Early Time Series Classification - Average Ouput KNN.ipynb'\n","!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1tS6nonHF9u"},"outputs":[],"source":["!git config --global user.email \"aditya.gupta18@imperial.ac.uk\"\n","!git config --global user.name \"adityag16\"\n","\n","!git commit -m \"All documentation done -- need to sort out file paths\"\n","!git push origin main"]},{"cell_type":"code","source":[""],"metadata":{"id":"G7t1wmFYu_q4"},"execution_count":null,"outputs":[]}]}
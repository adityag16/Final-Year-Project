{"cells":[{"cell_type":"markdown","metadata":{"id":"XVaAULW6qhh1"},"source":["### Connect Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DdiqzlkZMhe"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive') "]},{"cell_type":"markdown","metadata":{"id":"ttpluWU4tHLq"},"source":["### Package Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_3NHgGwZIsI"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import pandas as pd\n","import tensorflow as tf\n","from scipy.signal import savgol_filter\n","from collections import Counter\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation\n","from scipy.spatial import distance\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import seaborn as sns\n","from scipy.optimize import curve_fit\n","from scipy.signal import filtfilt\n","from collections import defaultdict"]},{"cell_type":"markdown","metadata":{"id":"6cosBM9Jd74f"},"source":["### GPU Device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbMtBMv4Rxa5"},"outputs":[],"source":["!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLu_lZGKu9dp"},"outputs":[],"source":["gpu = tf.test.gpu_device_name()\n","print(gpu)"]},{"cell_type":"markdown","metadata":{"id":"ihJkU1v2STVo"},"source":["### Pre-Processing Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1nRYshEtGy2"},"outputs":[],"source":["def decaying_exp(x, a, b):\n","    \"\"\" Returns exponential function\n","\n","    Parameters\n","    ----------\n","    x : ndarray\n","        times\n","    a : double\n","        t(inf) value\n","    b : double\n","        slope to t=0\n","        \n","    Returns\n","    -------\n","    ndarray\n","        y-axis values of the function\n","    \"\"\"\n","    return a*(1-np.exp(-b * x))\n","\n","\n","def fit_pixels_interpolate(time, X, interpolate_idx):\n","    \"\"\" Interpolates the curves for each pixel\n","\n","    Parameters\n","    ----------\n","    time : ndarray\n","        times\n","    X : ndarray\n","        TxNM array to be interpolated\n","    idx_active : ndarray\n","        NM array specifying pixels that are active\n","    interpolate_idx : int\n","        interpolation is performed until this index\n","\n","    Returns\n","    -------\n","    popt : ndarray\n","        optimal parameters for interpolation of each pixel, with shape 2xNM\n","    \"\"\"\n","    popt = np.zeros((2, X.shape[1]))\n","\n","    # for every pixel\n","    for i in range(X.shape[1]):\n","\n","      data = filtfilt(b=np.ones(10) / 10, a=[1], x=X[:, i])\n","\n","      # Fit the curve (interpolate) to the decaying exponential\n","      try:\n","        popt[:, i], pcov = curve_fit(decaying_exp, time[:interpolate_idx], data[:interpolate_idx], p0=[-10, 0.1])\n","      except:\n","        \n","        popt[:, i] = None\n","\n","    return popt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9gzaEl7wE7H"},"outputs":[],"source":["def filter_by_drift(df, interpolate_idx):\n","\n","  \"\"\" Filters pixels by their fitting to the drift model\n","  \n","  Parameters\n","  ----------\n","  df : pandas.DataFrame \n","    DataFrame with pixel data that the fitting is applied to\n","  interpolate_idx : int\n","    Interpolation is perfromed until this index\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame with only the data from the active pixels\n","  drfit_avg : numpy.array\n","    Array containing the average drift value at each time stamp\n","\n","  \"\"\"\n","\n","  popt = fit_pixels_interpolate(np.array(df.index), df.values, interpolate_idx)\n","\n","  drift_avg = np.zeros(df.shape[0])\n","  pix_count = 0\n","  active = np.array(np.zeros(df.shape[1]), dtype=bool)\n","\n","  for idx in range(df.shape[1]):\n","\n","  # check if any of the drift params for the pixel are nan\n","    if(np.isnan(popt[0, idx]) and np.isnan(popt[1, idx])):\n","      active[idx] = False\n","    else:\n","      # if drift params exist then iterate over the values of the index and use these as x values for the drift curve\n","      y_vals = []\n","      for i in df.index:\n","        val = decaying_exp(i, popt[0,idx], popt[1,idx])\n","        y_vals.append(val)\n","      \n","      # subtract the extrapolated drift from the signal\n","      drift_error = np.abs(np.array(df.values[:, idx] - y_vals))\n","      \n","      # only keep pixels with drift error of less than 30mV\n","      if((drift_error < 30).all()):\n","        drift_avg = np.add(drift_avg, np.array(y_vals))\n","        pix_count += 1\n","        active[idx] = True\n","      else:\n","        active[idx] = False\n","\n","  drift_avg/=pix_count\n","\n","  df = df.loc[:, active]\n","\n","  return df, drift_avg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1YMbu9bSW5m"},"outputs":[],"source":["def filter_by_vref(X, v_thresh=70):\n","    '''\n","    Identifies active pixels by checking if one of the first 10 derivatives d(i) is > v_thresh\n","\n","    Parameters\n","    ---------\n","    X : ndarray\n","        Input 2D array (T x NM). T = time samples, NM = total number of pixels\n","    v_thresh : int, optional\n","        Minimum value of the derivative d(i)=X(i+1)-X(i) in mV. Default is 70\n","        \n","    Returns\n","    -------\n","    ndarray\n","        1D array of bool with dimension (NM). For each pixel, returns True if, during the first 10 samples,\n","        one of the derivatives is > v_thresh. The derivatives are calculated as d(i) = X(i+1)-X(i)\n","    '''\n","    return (np.diff(X[:10, :], axis=0) > v_thresh).any(axis=0)  # check if one of the first 10 derivatives is >v_thresh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjXkAhKwSgFB"},"outputs":[],"source":["def filter_by_vrange(X, v_range=(100, 900)):\n","    '''\n","    Identifies active pixels by checking that all the values are in v_range\n","\n","    Parameters\n","    ---------\n","    X : ndarray\n","        Input 2D array (T x NM). T = time samples, NM = total number of pixels\n","    v_range : (int, int), optional\n","        tuple containing the minimum and maximum allowable voltage in mV. Default is (100, 900)\n","        \n","    Returns\n","    -------\n","    ndarray\n","        1D array of bool with dimension (NM). For each pixel, returns True if the value is always in v_range\n","    '''\n","    return (X < v_range[1]).all(axis=0) & (X > v_range[0]).all(axis=0)  # for each pixel, check if all the values are\n","    # within the given range\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5a7Uqi9Skg_"},"outputs":[],"source":["def filter_by_derivative(X, vthresh=5):\n","    \"\"\" Identifies active pixels by checking that the absolute value of the derivative is always below vthresh\n","\n","    Parameters\n","    ----------\n","    X : ndarray\n","        input 2D array of shape TxNM\n","    vthresh : int\n","        threshold for active pixels. Default is 5\n","        \n","    Returns\n","    -------\n","    ndarray\n","        1D array of bool with dimension (NM). For each pixel, returns True if all the derivatives are below vthresh\n","    \"\"\"\n","    x_diff = np.abs(np.diff(X, axis=0))\n","    return (x_diff < vthresh).all(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhuoGbbPutKZ"},"outputs":[],"source":["def filter_active_pixels(df, v_thresh_ref=50, v_range=(100, 900), v_thresh_deriv=5): \n","  \"\"\" Filters pixels by reference electrode voltage, derivative and voltage range \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be filtered \n","  v_thresh_ref : int, optional\n","    Threshold for active pixels for filtering by reference electrode volatge . Default is 50\n","  v_range : (int, int), optional\n","    Tuple containing the minimum and maximum allowable voltage in mV for the voltage range filteration. Default is (100, 900)\n","  v_thresh_deriv : int, optional\n","    Threshold for filtering pixels by derivative. Default is 5\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after data from inactive pixels is removed\n","  \"\"\"\n","  \n","  active = filter_by_vref(df.values, v_thresh_ref) & filter_by_vrange(df.values, v_range) & filter_by_derivative(df.values, v_thresh_deriv)\n","  \n","  # drop pixels \n","  df = df.loc[: , active]\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZJkYzPiVvd6"},"outputs":[],"source":["def filter_active_pixels_deriv(df, v_thresh_deriv=5): \n","  \"\"\" Filters pixels by derivative  \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be filtered \n","  v_thresh_deriv : int, optional\n","    Threshold for filtering pixels by derivative. Default is 5\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after data from inactive pixels is removed\n","  \"\"\"\n","  \n","  active = filter_by_derivative(df.values, v_thresh_deriv)\n","  \n","  # drop pixels \n","  df = df.loc[: , active]\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imVXR8eVUrby"},"outputs":[],"source":["def filter_active_pixels_range(df, v_range=(100, 900)):\n","  \"\"\" Filters pixels by voltage range \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be filtered \n","  v_range : (int, int), optional\n","    Tuple containing the minimum and maximum allowable voltage in mV for the voltage range filteration. Default is (100, 900)\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after data from inactive pixels is removed\n","  \"\"\"\n","  \n","  active = filter_by_vrange(df.values, v_range)\n","\n","  # drop pixels \n","  df = df.loc[: , active]\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTF9Vh78MZSB"},"outputs":[],"source":["def reshape_data(df, rows, cols):\n","  \"\"\" Reshapes TxNM data into TxNxM where (T = Number of time samples, N = Number of Rows, M = Number of Columns)\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will be reshaped\n","  rows : int\n","    Number of rows \n","  cols : int\n","    Number of columns\n","\n","  Returns\n","  -------\n","  X : numpy.ndarray\n","    3D array containing the reshaped data \n","  \"\"\"\n","  X = df.values #pandas.DataFrame.values: Return a Numpy representation of the DataFrame.\n","  X = X.reshape(-1, rows, cols, order='F') #or C. different reshaping row by row or column by column but this works\n","  return X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9Xt8X4zL7hc"},"outputs":[],"source":["def filter_chemical_pixels(df, arr_rows, arr_cols):\n","  \"\"\" Removes all the temperature pixels from the data \n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will filtered\n","  rows : int\n","    Number of rows \n","  cols : int\n","    Number of columns\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame after temperature pixels are removed\n","  \"\"\"\n","  X = reshape_data(df, arr_rows, arr_cols) # reshape data to T x 78 x 56\n","  X_mean = np.mean(X, axis=0) # get mean to have 78 x 56 shape\n","  X_mean[1::3, 1::3] = np.nan # set temperature pixels to nan\n","  X_mean = X_mean.flatten('F') # restore shape to 4068 \n","\n","  active_chemical = ~(np.isnan(X_mean)) # get bool array of all chemical pixels\n","\n","  # drop pixels \n","  df = df.loc[: , active_chemical]\n","  return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o82EQTYe9euH"},"outputs":[],"source":["def time_to_index(times, time_vect):\n","    '''\n","    Returns index of the times closest to the desired ones time_vect\n","\n","    Parameters\n","    ---------\n","    times : list\n","        list of integers containing the desired times\n","    time_vect : nparray\n","        array of the times at which the values are sampled\n","\n","    Returns\n","    -------\n","    list\n","        for each element in the input list times, return an element in the output list\n","        with the index of the sample closest to the desired time\n","    '''\n","    indices = []\n","    for time in times:  # for each time in the input list\n","        indices.append( np.argmin(np.abs(time_vect - time)) )\n","        # find index of the sampled time (in time_vect) closest to the desired one (time)\n","    return indices\n","\n","\n","def find_loading_time(time_vect, X, bounds=(600, 900)):  # for v2\n","    ''' Finds loading and settling time for the data of v2 chip\n","\n","    Parameters\n","    ----------\n","    time_vect : ndarray\n","        1D array with dimension T containing the sampling times\n","    X : ndarray\n","        2D array with dimension TxNM containing the sampled data\n","    bounds : list, optional\n","        tuple containing the minimum and maximum times (in ms) where the loading time has to be searched.\n","        Default is (600, 900)\n","        \n","    Returns\n","    -------\n","    tuple\n","        - settled_index : index at which the settling occurs\n","        - settled_time : time at which the settling occurs\n","    '''\n","\n","    search_start, search_end = time_to_index(bounds, time_vect)  # for each time in bounds, find the index\n","    # of the sample (in time_vect) that is closest to the desired one (in bounds)\n","    X_mean = np.mean(X, axis=1)  # for each sample, calculate the mean of all pixels\n","    X_mean_diff = np.diff(X_mean)  # find the derivative\n","\n","    loading_index = np.argmax(X_mean_diff[search_start:search_end]) + search_start + 1  # find the index\n","    # where the derivative is max in the specified interval\n","    loading_index = loading_index  # add settling time\n","    settled_index = loading_index + 10  # add settling time\n","    settled_time = time_vect[settled_index]  # find the time that index corresponds to\n","\n","    return settled_index, settled_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9m8OqTUtQVb0"},"outputs":[],"source":["def preprocess_data(df, deriv_thresh, deriv_thresh_bgsub=5):\n","  \"\"\"Applies all pre-processing steps to single well experimental data\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will pre-processed\n","  deriv_thresh : int\n","    Threshold for filtering by derivative\n","  deriv_thresh_bgsub : int, optional\n","    Threshold for filtering by derivative after background subtraction step\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    Transpose of the DataFrame with only data from active pixels after pre-processing\n","  \"\"\"\n","\n","  df = filter_chemical_pixels(df, 78, 56) # filter all chemical pixels\n","\n","  df = filter_active_pixels_drop(df=df, v_thresh_deriv=deriv_thresh, v_range=(100,900)) # filter pixels by range, vref and deriv\n","\n","  settle_idx, settle_time = find_loading_time(df.index, df, bounds=(600, 900)) # find settling point\n","  df = df.iloc[settle_idx + 10:, :] # use only the data after the settling time + 30s to allow reaction to have settled\n","\n","  df = df.sub(df.iloc[0, :], axis='columns') # subtract value of first pixel from all pixels\n","\n","  if(len(filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub).columns) != 0): # check if there is still data present after filtering\n","    df = filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub) # if data is present do filtering otherwise don't\n","\n","  df = df.iloc[0:150+250, :] # take only 400 samples after settling point (approx 19-20mins) \n","\n","  df.index = df.index - df.index[0] # set the first time value to 0\n","  \n","  X, drift = filter_by_drift(df, 40) # filter by fitting of pixel to drift model\n","\n","  if(len(X.columns) != 0): \n","    df = X\n","    \n","  for col in df.columns:\n","    df[col] = savgol_filter(df[col],101, 3) # apply smoothing to each pixel \n","\n","  return df.T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9Er5B2BxpUK"},"outputs":[],"source":["def preprocess_partial_data(df, deriv_thresh, deriv_thresh_bgsub=5):\n","  \n","  \"\"\"Applies all pre-processing steps to double well experimental data\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","    Dataframe containing pixel data which will pre-processed\n","  deriv_thresh : int\n","    Threshold for filtering by derivative\n","  deriv_thresh_bgsub : int, optional\n","    Threshold for filtering by derivative after background subtraction step\n","\n","  Returns\n","  -------\n","  df : pandas.DataFrame\n","    DataFrame with only data from active pixels after pre-processing\n","  \"\"\"\n","\n","  df = filter_active_pixels_range(df=df, v_range=(100,900)) # filter by range incase of any saturation\n","  \n","  df = filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh) # filter pixels by deriv\n","\n","  df = df.sub(df.iloc[0, :], axis='columns') # subtract value of first pixel from all pixels\n","\n","  if(len(filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub).columns) != 0): # check if there is still data present after filtering\n","    df = filter_active_pixels_deriv(df=df, v_thresh_deriv=deriv_thresh_bgsub) # if data is present do filtering otherwise dont\n","\n","  df = df.iloc[:150+250, :] # take only 400 samples after settling point (approx 19-20mins) \n","\n","  df.index = df.index - df.index[0] # set the first time value to 0\n","  \n","  X, drift = filter_by_drift(df, 40) # filter by fitting of pixel to drift model\n","\n","  if(len(X.columns) != 0): \n","    df = X\n","\n","  for col in df.columns:\n","    df[col] = savgol_filter(df[col],101, 3) # apply smoothing to each pixel\n","    \n","  return df.T"]},{"cell_type":"markdown","metadata":{"id":"PaNIFO5iSa9C"},"source":["### Evaluation Metric Helper Functions"]},{"cell_type":"code","source":["def accuracy(classifications):\n","  \"\"\" Returns the value of the accuracy from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  accuracy : double\n","    Classification Accuracy \n","  \"\"\"\n","  total = len(classifications)\n","  total_correct = 0\n","  for i in classifications.values():\n","    \n","    if(i[0] == None or i[1] == None): ## if any predictions are inconclusive \n","      continue\n","\n","    if(i[0] == i[1]):\n","      total_correct +=1\n","\n","  accuracy = (total_correct/total)\n","\n","  return accuracy"],"metadata":{"id":"U2zoSqPJLatm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sensitivity(classifications):\n","  \"\"\" Returns the value of the sensitivity from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  sensitivity : double\n","    Classification sensitivity \n","  \"\"\"\n","  true_pos = 0\n","  false_neg = 0\n","\n","  for i in classifications.values():\n","\n","    true_label = int(i[1])\n","    predicted = int(i[0])\n","\n","    if(true_label == 1 and predicted == 1):\n","      true_pos += 1\n","    \n","    if(true_label == 1 and predicted == 0):\n","      false_neg += 1\n","\n","  sensitivity = (true_pos/(true_pos + false_neg))\n","\n","  return sensitivity"],"metadata":{"id":"lzSAF5WsTIuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def specificity(classifications):\n","  \"\"\" Returns the value of the specificity from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  specificity : double\n","    Classification specificity \n","  \"\"\"\n","\n","  true_neg = 0\n","  false_pos = 0\n","\n","  for i in classifications.values():\n","    true_label = int(i[1])\n","    predicted = int(i[0])\n","    \n","    if(true_label == 0 and predicted == 0):\n","      true_neg += 1\n","    \n","    if(true_label == 0 and predicted == 1):\n","      false_pos += 1\n","\n","  specificity = (true_neg/(true_neg + false_pos))\n","\n","  return specificity"],"metadata":{"id":"WP_kdiXMYeU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def precision(classifications):\n","  \"\"\" Returns the value of the precision from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  precision : double\n","    Classification precision \n","  \"\"\"\n","  true_pos = 0\n","  false_pos = 0\n","\n","  for i in classifications.values():\n","    true_label = int(i[1])\n","    predicted = int(i[0])\n","    \n","    if(true_label == 1 and predicted == 1):\n","      true_pos += 1\n","    \n","    if(true_label == 0 and predicted == 1):\n","      false_pos += 1\n","\n","  precision = (true_pos/(true_pos + false_pos))\n","\n","  return precision"],"metadata":{"id":"w7-_ZPDDaxRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f1(classifications):\n","  \"\"\" Returns the value of the F1 score from predicted outputs and true outputs\n","\n","  Parameters\n","  ----------\n","  classifications : dictionary(tuple(int,int))\n","    Dictionary containing a tuple which holds the true output and prediction output from classification\n","\n","  Returns\n","  -------\n","  double\n","    Classification F1 score \n","  \"\"\"\n","  numerator = 2*precision(classifications)*sensitivity(classifications)\n","  denominator = precision(classifications) + sensitivity(classifications)\n","  return numerator/denominator"],"metadata":{"id":"qkFpU-UJbV1R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mp6t9KbXUtag"},"source":["### Data Loading Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72Rhya-GUxAS"},"outputs":[],"source":["def load_partial_covid_exp(filepath):\n","  \"\"\" Loading in double well experimental data from csv file\n","\n","  Parameters\n","  ----------\n","  filepath : string\n","    Path to the csv file that loads the double well data\n","  \n","  Returns\n","  -------\n","  df_pos : pandas.DataFrame\n","    DataFrame with pixel data from the positive well \n","  df_neg : panads.DataFrame\n","    DataFrame with pixel data from the negative well\n","  \"\"\"\n","  \n","  bot_filepath = filepath[:-4] + \"_bot.csv\"\n","  top_filepath = filepath[:-4] + \"_top.csv\"\n","\n","  ## load in 2 sheets\n","  df_neg = pd.read_csv(top_filepath, header=0, index_col=0)\n","  df_pos = pd.read_csv(bot_filepath, header=0, index_col=0)\n","\n","  return df_pos, df_neg"]},{"cell_type":"markdown","metadata":{"id":"W9kgS_-Cx1nm"},"source":["### Array Dims"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whsJZh4Zx0xs"},"outputs":[],"source":["arr_rows = 78\n","arr_cols = 56"]},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"KCr7gvB_tf5-"}},{"cell_type":"markdown","source":["#### Positive Samples"],"metadata":{"id":"AvJiLnQ8tiKx"}},{"cell_type":"code","source":["## Average pixel value for all samples \n","\n","with tf.device(gpu):\n","  ## Gamma 1\n","  avg_data_g1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma1.app.1e5/gamma1.app.1e5_data_export.csv\"\n","  avg_g1 = pd.read_csv(avg_data_g1_file, header=0)\n","\n","  ## Gamma 2\n","  avg_data_g2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma2.app.1e4/gamma2.app.1e4_data_export.csv\"\n","  avg_g2 = pd.read_csv(avg_data_g2_file, header=0)\n","\n","  ## Gamma 3\n","  avg_data_g3_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma3.app.1e5/gamma3.app.1e5_data_export.csv\"\n","  avg_g3 = pd.read_csv(avg_data_g3_file, header=0)\n","  \n","  ## Gamma 5 \n","  avg_data_g5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma5.app.1e4/gamma5.app.1e4_data_export.csv\"\n","  avg_g5 = pd.read_csv(avg_data_g5_file, header=0)\n","\n","  ## 22RV1.ap1\n","  avg_data_22rv1_ap1_file = \"/DNAPositives/22RV1.ap1/22RV1.ap1_data_export.csv\"\n","  avg_22rv1_ap1 = pd.read_csv(avg_data_22rv1_ap1_file, header=0)\n","\n","  ## 22RV1.ap2\n","  avg_data_22rv1_ap2_file = \"/DNAPositives/22RV1.ap2/22RV1.ap2_data_export.csv\"\n","  avg_22rv1_ap2 = pd.read_csv(avg_data_22rv1_ap2_file, header=0)\n","\n","  ## 22RV1y.p3\n","  avg_data_22rv1y_p3_file = \"/DNAPositives/22Rv1y.p3/22Rv1y.p3_data_export.csv\"\n","  avg_22rv1y_p3 = pd.read_csv(avg_data_22rv1y_p3_file, header=0)\n","\n","  ## 22RV1y.p4\n","  avg_data_22rv1y_p4_file = \"/DNAPositives/22Rv1y.p4/22Rv1y.p4_data_export.csv\"\n","  avg_22rv1y_p4 = pd.read_csv(avg_data_22rv1y_p4_file, header=0)\n","\n","  ## ARV7.p1\n","  avg_data_arv7_p1_file = \"/DNAPositives/ARV7.p1/ARV7.p1_data_export.csv\"\n","  avg_arv7_p1 = pd.read_csv(avg_data_arv7_p1_file, header=0).iloc[1:, :].reset_index(drop=True) # row 0 was NAN\n","\n","  ## ARV7.p3\n","  avg_data_arv7_p3_file = \"/DNAPositives/ARV7.p3/ARV7.p3_data_export.csv\"\n","  avg_arv7_p3 = pd.read_csv(avg_data_arv7_p3_file, header=0)\n","\n","  ## ARV7.p4\n","  avg_data_arv7_p4_file = \"/DNAPositives/ARV7.p4/ARV7.p4_data_export.csv\"\n","  avg_arv7_p4 = pd.read_csv(avg_data_arv7_p4_file, header=0)\n","\n","  ## Beta 1\n","  avg_data_b1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta1.app.1e4/beta1.app.1e4_data_export.csv\"\n","  avg_b1 = pd.read_csv(avg_data_b1_file, header=0)\n","\n","  ## Beta 2\n","  avg_data_b2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta2.app.1e5/beta2.app.1e5_data_export.csv\"\n","  avg_b2 = pd.read_csv(avg_data_b2_file, header=0)\n","\n","  ## Beta 5\n","  avg_data_b5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta5.app.1e5/beta5.app.1e5_data_export.csv\"\n","  avg_b5 = pd.read_csv(avg_data_b5_file, header=0)\n","  "],"metadata":{"id":"Ekqd_pB0tuTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## All pixel values for each time stamp\n","\n","with tf.device(gpu):\n","  ## Gamma 1\n","  g1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma1.app.1e5/gamma1.app.1e5_vsChem_export.csv\"\n","  g1 = pd.read_csv(g1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g1.index = avg_g1[\"Time Elapsed\"]\n","\n","  ## Gamma 2\n","  g2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma2.app.1e4/gamma2.app.1e4_vsChem_export.csv\"\n","  g2 = pd.read_csv(g2_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g2.index = avg_g2[\"Time Elapsed\"]\n","\n","  ## Gamma 3\n","  g3_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma3.app.1e5/gamma3.app.1e5_vsChem_export.csv\"\n","  g3 = pd.read_csv(g3_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g3.index = avg_g3[\"Time Elapsed\"]\n","\n","  ## Gamma 5\n","  g5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/gamma5.app.1e4/gamma5.app.1e4_vsChem_export.csv\"\n","  g5 = pd.read_csv(g5_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  g5.index = avg_g5[\"Time Elapsed\"]\n","\n","  ## 22RV1.ap1\n","  rv1_ap1_file = \"/DNAPositives/22RV1.ap1/22RV1.ap1_vsChem_export.csv\"\n","  rv1_ap1 = pd.read_csv(rv1_ap1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1_ap1.index = avg_22rv1_ap1['Time Elapsed']\n","\n","  ## 22RV1.ap2\n","  rv1_ap2_file = \"/DNAPositives/22RV1.ap2/22RV1.ap2_vsChem_export.csv\"\n","  rv1_ap2 = pd.read_csv(rv1_ap2_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1_ap2.index = avg_22rv1_ap2['Time Elapsed']\n","\n","  ## 22RV1y.p3\n","  rv1y_p3_file = \"/DNAPositives/22Rv1y.p3/22Rv1y.p3_vsChem_export.csv\"\n","  rv1y_p3 = pd.read_csv(rv1y_p3_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1y_p3.index = avg_22rv1y_p3['Time Elapsed']\n","\n","  ## 22RV1y.p4\n","  rv1y_p4_file = \"/DNAPositives/22Rv1y.p4/22Rv1y.p4_vsChem_export.csv\"\n","  rv1y_p4 = pd.read_csv(rv1y_p4_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  rv1y_p4.index = avg_22rv1y_p4['Time Elapsed']\n","\n","  ## ARV7.p1 \n","  arv7_p1_file = \"/DNAPositives/ARV7.p1/ARV7.p1_vsChem_export.csv\"\n","  arv7_p1 = pd.read_csv(arv7_p1_file, header=None).iloc[:, :(arr_rows*arr_cols)] \n","  arv7_p1.index = avg_arv7_p1[\"Time Elapsed\"]\n","\n","  ## ARV7.p3 \n","  arv7_p3_file = \"/DNAPositives/ARV7.p3/ARV7.p3_vsChem_export.csv\"\n","  arv7_p3 = pd.read_csv(arv7_p3_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv7_p3.index = avg_arv7_p3[\"Time Elapsed\"]\n","\n","  ## ARV7.p4 \n","  arv7_p4_file = \"/DNAPositives/ARV7.p4/ARV7.p4_vsChem_export.csv\"\n","  arv7_p4 = pd.read_csv(arv7_p4_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv7_p4.index = avg_arv7_p4[\"Time Elapsed\"]\n","\n","  ## Beta 1\n","  b1_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta1.app.1e4/beta1.app.1e4_vsChem_export.csv\"\n","  b1 = pd.read_csv(b1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  b1.index = avg_b1[\"Time Elapsed\"]\n","\n","  ## Beta 2\n","  b2_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta2.app.1e5/beta2.app.1e5_vsChem_export.csv\"\n","  b2 = pd.read_csv(b2_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  b2.index = avg_b2[\"Time Elapsed\"]\n","\n","  ## Beta 5\n","  b5_file = \"/DNAPositives/100921_DNA/100921_DNA/Data/beta5.app.1e5/beta5.app.1e5_vsChem_export.csv\"\n","  b5 = pd.read_csv(b5_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  b5.index = avg_b5[\"Time Elapsed\"]"],"metadata":{"id":"vZRah5zpxXp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Negative Samples"],"metadata":{"id":"7qOF9VBstkbe"}},{"cell_type":"code","source":["## Average pixel value for all samples \n","\n","with tf.device(gpu):  \n","  ## ARV7.n1\n","  avg_data_arv7_file = \"/DNANegatives/ARV7.n1/ARV7.n1_data_export.csv\"\n","  avg_arv7 = pd.read_csv(avg_data_arv7_file, header=0)\n","\n","  ## Yap.n2\n","  avg_data_yap_file = \"/DNANegatives/yap.n2/yap.n2_data_export.csv\"\n","  avg_yap = pd.read_csv(avg_data_yap_file, header=0)\n","\n","  ## Yap1.n2\n","  avg_data_yap1_file = \"/DNANegatives/yap1.n2/yap1.n2_data_export.csv\"\n","  avg_yap1 = pd.read_csv(avg_data_yap1_file, header=0).iloc[1:, :].reset_index() # row 0 was NAN\n","\n","  ## Yap1.n1.1 \n","  avg_data_yap1n1_file = \"/DNANegatives/yap1.n1.1/yap1.n1.1_data_export.csv\"\n","  avg_yap1n1 = pd.read_csv(avg_data_yap1n1_file, header=0).iloc[1:, :].reset_index() # row 0 was NAN\n","\n","  ## ARV7.n2\n","  avg_data_arv72_file = \"/DNANegatives/ARV7.n2/ARV7.n2_data_export.csv\"\n","  avg_arv72 = pd.read_csv(avg_data_arv72_file, header=0)\n","\n","  ## ARV7.n3\n","  avg_data_arv73_file = \"/DNANegatives/ARV7.n3/ARV7.n3_data_export.csv\"\n","  avg_arv73 = pd.read_csv(avg_data_arv73_file, header=0)\n","\n","  ## DU145y.n1\n","  avg_data_du145y_n1_file = \"/DNANegatives/DU145y.n1/DU145y.n1_data_export.csv\"\n","  avg_du145y_n1 = pd.read_csv(avg_data_du145y_n1_file, header=0)"],"metadata":{"id":"mlU83yKsuSHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## All pixel values for each time stamp\n","\n","with tf.device(gpu):   \n","  ## ARV7.n1 \n","  arv7_file = \"/DNANegatives/ARV7.n1/ARV7.n1_vsChem_export.csv\"\n","  arv7 = pd.read_csv(arv7_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv7.index = avg_arv7[\"Time Elapsed\"]\n","\n","  ## Yap.n2\n","  yap_file = \"/DNANegatives/yap.n2/yap.n2_vsChem_export.csv\"\n","  yap = pd.read_csv(yap_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  yap.index = avg_yap[\"Time Elapsed\"]\n","\n","  ## Yap1.n2\n","  yap1_file = \"/DNANegatives/yap1.n2/yap1.n2_vsChem_export.csv\"\n","  yap1 = pd.read_csv(yap1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  yap1.index = avg_yap1[\"Time Elapsed\"]\n","\n","  ## Yap1.n1.1\n","  yap1n1_file = \"/DNANegatives/yap1.n1.1/yap1.n1.1_vsChem_export.csv\"\n","  yap1n1 = pd.read_csv(yap1n1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  yap1n1.index = avg_yap1n1[\"Time Elapsed\"]\n","\n","  ## ARV7.n2\n","  arv72_file = \"/DNANegatives/ARV7.n2/ARV7.n2_vsChem_export.csv\"\n","  arv72 = pd.read_csv(arv72_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv72.index = avg_arv72[\"Time Elapsed\"]\n","\n","  ## ARV7.n3\n","  arv73_file = \"/DNANegatives/ARV7.n3/ARV7.n3_vsChem_export.csv\"\n","  arv73 = pd.read_csv(arv73_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  arv73.index = avg_arv73[\"Time Elapsed\"]\n","\n","  ## DU145y.n1\n","  du145y_n1_file = \"/DNANegatives/DU145y.n1/DU145y.n1_vsChem_export.csv\"\n","  du145y_n1 = pd.read_csv(du145y_n1_file, header=None).iloc[:, :(arr_rows*arr_cols)]\n","  du145y_n1.index = avg_du145y_n1[\"Time Elapsed\"]"],"metadata":{"id":"W3_XExOjypwI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Partial Covid Data"],"metadata":{"id":"yjXPLEfmRUJH"}},{"cell_type":"code","source":["## 150520_2_118\n","avg_118_file = \"/COVIDPartialData/150520_2_118/exp_summary_118.csv\"\n","exp_118_pos, exp_118_neg = load_partial_covid_exp(avg_118_file)\n","\n","## 150520_4_2_86\n","avg_86_file = \"/COVIDPartialData/150520_4_2_86/exp_summary_86.csv\"\n","exp_86_pos, exp_86_neg = load_partial_covid_exp(avg_86_file)\n","\n","## 150520_5_129\n","avg_129_file = \"/COVIDPartialData/150520_5_129/exp_summary_129.csv\"\n","exp_129_pos, exp_129_neg = load_partial_covid_exp(avg_129_file)\n","\n","## 180520_4_165\n","avg_165_file = \"/COVIDPartialData/180520_4_165/exp_summary_165.csv\"\n","exp_165_pos, exp_165_neg = load_partial_covid_exp(avg_165_file)\n","\n","## 180520_6_35\n","avg_35_file = \"/COVIDPartialData/180520_6_35/exp_summary_35.csv\"\n","exp_35_pos, exp_35_neg = load_partial_covid_exp(avg_35_file)\n","\n","## 190520_1_28\n","avg_28_file = \"/COVIDPartialData/190520_1_28/exp_summary_28.csv\"\n","exp_28_pos, exp_28_neg = load_partial_covid_exp(avg_28_file) \n","\n","## 190520_2_14\n","avg_14_file = \"/COVIDPartialData/190520_2_14/exp_summary_14.csv\"\n","exp_14_pos, exp_14_neg = load_partial_covid_exp(avg_14_file)\n","\n","## 210520_2_40\n","avg_40_file = \"/COVIDPartialData/210520_2_40/exp_summary_40.csv\"\n","exp_40_pos, exp_40_neg = load_partial_covid_exp(avg_40_file)\n","\n","## 210520_3_88\n","avg_88_file = \"/COVIDPartialData/210520_3_88/exp_summary_88.csv\"\n","exp_88_pos, exp_88_neg = load_partial_covid_exp(avg_88_file)\n","\n","## 210520_6_27\n","avg_27_file = \"/COVIDPartialData/210520_6_27/exp_summary_27.csv\"\n","exp_27_pos, exp_27_neg = load_partial_covid_exp(avg_27_file)\n","\n","## 250520_1_134\n","avg_134_file = \"/COVIDPartialData/250520_1_134/exp_summary_134.csv\"\n","exp_134_pos, exp_134_neg = load_partial_covid_exp(avg_134_file)\n","\n","## 250520_2_97\n","avg_97_file = \"/COVIDPartialData/250520_2_97/exp_summary_97.csv\"\n","exp_97_pos, exp_97_neg = load_partial_covid_exp(avg_97_file)\n","\n","## 250520_6_2D1\n","avg_2d1_file = \"/COVIDPartialData/250520_6_2D1/exp_summary_2D1.csv\"\n","exp_2d1_pos, exp_2d1_neg = load_partial_covid_exp(avg_2d1_file)\n","\n","## 250520_7_64\n","avg_64_file = \"/COVIDPartialData/250520_7_64/exp_summary_64.csv\"\n","exp_64_pos, exp_64_neg = load_partial_covid_exp(avg_64_file)"],"metadata":{"id":"ORRtMFfEZBwV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XgnkewwwPki"},"source":["### Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"CTcUwvRiwUmJ"},"source":["#### Positive Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-WlDoK49D2Y"},"outputs":[],"source":["g1 = preprocess_data(g1, 500)\n","g2 = preprocess_data(g2, 500) #-- 5.2865965366363525\n","g3 = preprocess_data(g3, 500) #-- 8.670833349227905\n","g5 = preprocess_data(g5, 500) #-- 5.734365940093994\n","rv1_ap1 = preprocess_data(rv1_ap1, 500)\n","rv1_ap2 = preprocess_data(rv1_ap2, 500) #-- 6.83215594291687\n","rv1y_p3 = preprocess_data(rv1y_p3, 500) \n","rv1y_p4 = preprocess_data(rv1y_p4, 500)\n","arv7_p1 = preprocess_data(arv7_p1, 500)\n","arv7_p3 = preprocess_data(arv7_p3, 500)\n","arv7_p4 = preprocess_data(arv7_p4, 500)\n","b1 = preprocess_data(b1, 500) #-- 7.712360858917236\n","b2 = preprocess_data(b2, 500) #-- 7.934495210647583\n","b5 = preprocess_data(b5, 500)"]},{"cell_type":"markdown","metadata":{"id":"1WaPBFGuwYN4"},"source":["#### Negative Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gazhgzLT9HLV"},"outputs":[],"source":["arv7 = preprocess_data(arv7, 500)\n","yap = preprocess_data(yap, 500) #--8.715267419815063\n","yap1 = preprocess_data(yap1, 500)\n","yap1n1 = preprocess_data(yap1n1, 500)\n","arv72 = preprocess_data(arv72, 500)\n","arv73 = preprocess_data(arv73, 500)\n","du145y_n1 = preprocess_data(du145y_n1, 500)"]},{"cell_type":"markdown","metadata":{"id":"nUwBPNQNjQ7w"},"source":["#### Covid Partial Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQBQ_1YF9Oqj"},"outputs":[],"source":["exp_118_pos = preprocess_partial_data(exp_118_pos, 500)\n","exp_86_pos = preprocess_partial_data(exp_86_pos, 500)\n","exp_129_pos = preprocess_partial_data(exp_129_pos, 500)\n","exp_165_pos = preprocess_partial_data(exp_165_pos, 500)\n","exp_35_pos = preprocess_partial_data(exp_35_pos, 500)\n","exp_28_pos = preprocess_partial_data(exp_28_pos, 500)\n","exp_14_pos = preprocess_partial_data(exp_14_pos, 500)\n","exp_40_pos = preprocess_partial_data(exp_40_pos, 500)\n","exp_88_pos = preprocess_partial_data(exp_88_pos, 500)\n","exp_27_pos = preprocess_partial_data(exp_27_pos, 500)\n","exp_134_pos = preprocess_partial_data(exp_134_pos, 500)\n","exp_97_pos = preprocess_partial_data(exp_97_pos, 500)\n","exp_2d1_pos = preprocess_partial_data(exp_2d1_pos, 500)\n","exp_64_pos = preprocess_partial_data(exp_64_pos, 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYsOnsAW9Rob"},"outputs":[],"source":["exp_118_neg = preprocess_partial_data(exp_118_neg, 500)\n","exp_86_neg = preprocess_partial_data(exp_86_neg, 500)\n","exp_129_neg = preprocess_partial_data(exp_129_neg, 500)\n","exp_165_neg = preprocess_partial_data(exp_165_neg, 500)\n","exp_35_neg = preprocess_partial_data(exp_35_neg, 500)\n","exp_28_neg = preprocess_partial_data(exp_28_neg, 500)\n","exp_14_neg = preprocess_partial_data(exp_14_neg, 500)\n","exp_40_neg = preprocess_partial_data(exp_40_neg, 500)\n","exp_88_neg = preprocess_partial_data(exp_88_neg, 500)\n","exp_27_neg = preprocess_partial_data(exp_27_neg, 500)\n","exp_134_neg = preprocess_partial_data(exp_134_neg, 500)\n","exp_97_neg = preprocess_partial_data(exp_97_neg, 500)\n","exp_2d1_neg = preprocess_partial_data(exp_2d1_neg, 500)\n","exp_64_neg = preprocess_partial_data(exp_64_neg, 500)"]},{"cell_type":"markdown","metadata":{"id":"cco-BOwij9af"},"source":["### Machine Learning - Neural Network Ensemble"]},{"cell_type":"markdown","metadata":{"id":"j5qgb5mx3rOW"},"source":["#### Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIYXEisg2WX_"},"outputs":[],"source":["def get_training_data_nn(positive_samples, negative_samples, timestamp, test_samples=[]):\n","  \"\"\" Gets the training data for the neural network classifier at a given timestamp\n","\n","  Parameters\n","  ----------\n","  positive_samples : dict(pandas.DataFrame)\n","    Dictionary of DataFrames with data from all the positive experiements\n","  negative_samples : dict(pandas.DataFrame)\n","    Dictionary of DataFrames with data from all the negative experiements\n","  timestamp : int\n","    Number of data-points which the data for each experiement must be truncated to\n","  test_samples : array, optional\n","    Array containing the name (key used in the dictionary) of the test samples, Default is []\n","\n","  Returns\n","  -------\n","  numpy.ndarray\n","    Contains the training data for the classifier at the given time stamp\n","  training_labels : numpy.array\n","    1D array with the true labels for each of the training experiements\n","  \"\"\"\n","  training_data = []\n","  pos_count = 0\n","  neg_count = 0\n","\n","  ## iterate postive samples dict\n","  for key, sample in positive_samples.items():\n","    \n","    ## if dataset is test data do not add to training set\n","    if(key in test_samples):\n","      continue\n","\n","    ## truncate sample to length t = timestamp (keep all rows and turncate columns)\n","    pos_subsample = sample.to_numpy()[:, 0:timestamp]\n","\n","    ## pos_count = 0 means this is first sample to set training data = sample ortherwise update training data \n","    if(pos_count == 0):\n","      training_data = pos_subsample\n","    else:\n","      training_data = np.concatenate((training_data,pos_subsample))\n","\n","    ## increment count of number of positive samples\n","    pos_count += len(sample)\n","\n","  ## iterate negative samples dict\n","  for key, sample in negative_samples.items():\n","    \n","    ## if dataset is test data do not add to training set\n","    if(key in test_samples):\n","      continue\n","\n","    ## truncate sample to length t = timestamp (keep all rows and turncate columns)\n","    neg_subsample = sample.to_numpy()[:, 0:timestamp]\n","\n","    ## update training data\n","    training_data = np.concatenate((training_data,neg_subsample))\n","\n","    ## increment count of number of negative samples\n","    neg_count += len(sample)\n","\n","  ## create positive and negative (1 and 0) label based on sample \n","  pos_labels = np.ones(pos_count)\n","  neg_labels = np.zeros(neg_count)\n","\n","  ## concatenate labels for final training labels\n","  training_labels = np.concatenate((pos_labels, neg_labels), axis=0)\n","\n","  return np.asarray(training_data), training_labels ## np.asarry() converts list to 2D np array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xH5J1l0cgIHI"},"outputs":[],"source":["def get_test_data_nn(sample, timestamp):\n","  \"\"\" Gets the test data for the neural network classifier at a given timestamp for a test sample\n","\n","  Parameters\n","  ----------\n","  sample : pandas.DataFrame\n","    The test sample to be used \n","  timestamp : int\n","    Number of data-points which the data for each experiement must be truncated to\n","\n","  Returns\n","  -------\n","  numpy.ndarray\n","    Contains the test data for the classifier at the given time stamp\n","  \"\"\"\n","  subsample = sample.to_numpy()[:, 0:timestamp]\n","\n","  return np.asarray(subsample)"]},{"cell_type":"markdown","metadata":{"id":"d6Qyl80Wzy-r"},"source":["#### Training Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-bA8RfjcM35"},"outputs":[],"source":["positives = {\"exp_118_pos\":exp_118_pos, \"exp_86_pos\":exp_86_pos,\"exp_129_pos\":exp_129_pos, \"exp_165_pos\":exp_165_pos, \n","             \"exp_35_pos\":exp_35_pos, \"exp_28_pos\":exp_28_pos, \"exp_14_pos\":exp_14_pos, \"exp_40_pos\":exp_40_pos, \n","             \"exp_88_pos\":exp_88_pos, \"exp_27_pos\":exp_27_pos, \n","             \"exp_134_pos\":exp_134_pos, \"exp_97_pos\":exp_97_pos, \"exp_2d1_pos\":exp_2d1_pos, \"exp_64_pos\":exp_64_pos, \n","             \"g1\":g1, \"g2\":g2, \"g3\":g3, \"g5\":g5, \"rv1_ap1\":rv1_ap1, \"rv1_ap2\":rv1_ap2,  \n","             \"arv7_p3\":arv7_p3,\"rv1y_p3\":rv1y_p3, \"rv1y_p4\":rv1y_p4, \n","             \"arv7_p1\":arv7_p1, \"arv7_p4\":arv7_p4, \"b1\":b1, \"b2\":b2, \"b5\":b5}\n","\n","negatives = {\"exp_118_neg\":exp_118_neg, \"exp_86_neg\":exp_86_neg, \"exp_129_neg\":exp_129_neg, \"exp_165_neg\":exp_165_neg, \n","             \"exp_35_neg\":exp_35_neg, \"exp_28_neg\":exp_28_neg, \"exp_14_neg\":exp_14_neg, \"exp_40_neg\":exp_40_neg, \n","             \"exp_88_neg\":exp_88_neg, \"exp_27_neg\":exp_27_neg, \"exp_134_neg\":exp_134_neg, \"exp_97_neg\":exp_97_neg, \n","             \"exp_2d1_neg\":exp_2d1_neg, \"exp_64_neg\":exp_64_neg, \"yap\":yap, \"yap1\":yap1, \"yap1n1\":yap1n1, \"arv72\":arv72, \n","             \"arv73\":arv73, \"du145y_n1\":du145y_n1, \"arv7\":arv7}"]},{"cell_type":"markdown","metadata":{"id":"rxkmk6GHqC7g"},"source":["#### Model Specs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eztwFZUaloVP"},"outputs":[],"source":["number_of_samples = len(g1.columns)\n","number_of_classifiers = 50\n","\n","timestep = int(number_of_samples/number_of_classifiers)\n","timestamps = [*range(timestep, number_of_samples+timestep, timestep)]\n","\n","batch_size = 250\n","epochs = 15\n","loss_function = 'binary_crossentropy'\n","optimiser = 'adam'"]},{"cell_type":"markdown","metadata":{"id":"qG3eDbNkqG9A"},"source":["#### Creating Ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVVPVw4-ndtu"},"outputs":[],"source":["def create_ensemble(number_of_classifiers, batch_size, epochs, loss_function, optimiser, timestamps, test_samples, positives, negatives):\n","  \"\"\" Makes and trains an ensemble of neural networks\n","\n","  Parameters\n","  ----------\n","  number_of_classifiers : int\n","    Number of classifers in the ensemble \n","  batch_size : int\n","    Number of experiements per batch during training\n","  epochs : int\n","    Number of epochs trained for\n","  loss_function : string\n","    Function used to calculate model loss\n","  optimiser : string\n","    Optimiser used during training\n","  timestamps : array(int)\n","    Array of timestamps at which the predictions are made\n","  test_samples : string\n","    The name of the experiements to used as the test samples\n","\n","  Returns\n","  -------\n","  neural_nets : array\n","    Array of trained neural networks\n","  \"\"\"\n","  neural_nets = [0]*number_of_classifiers\n","\n","  for i in range(number_of_classifiers):\n","\n","    ## make model \n","    neural_nets[i] = Sequential()\n","    neural_nets[i].add(Dense(32, activation='relu', input_dim = timestamps[i]))\n","    neural_nets[i].add(Dense(64, activation='relu'))\n","    neural_nets[i].add(Dense(128, activation='relu'))\n","    neural_nets[i].add(Dense(512, activation='relu'))\n","    neural_nets[i].add(Dense(1024, activation='relu'))\n","    neural_nets[i].add(Dense(2048, activation='relu'))\n","    neural_nets[i].add(Dense(1, activation='sigmoid'))\n","\n","    ## compile model \n","    neural_nets[i].compile(loss=loss_function, optimizer=optimiser, metrics=['accuracy'])\n","\n","    ## training data\n","    training_data, training_label = get_training_data_nn(positive_samples=positives, negative_samples=negatives, timestamp=timestamps[i], test_samples=[test_samples])\n","\n","    ## train model\n","    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n","    neural_nets[i].fit(training_data, training_label,  batch_size=batch_size, epochs=epochs, shuffle=True, callbacks=[callback], verbose=0)\n","\n","    # print(\"\\n\\n\")\n","\n","  return neural_nets"]},{"cell_type":"markdown","metadata":{"id":"fGH97jNBfzdu"},"source":["#### Evaluating Ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7K7KpmnZU0s"},"outputs":[],"source":["def get_time_index(timestamps, predictions):\n","  \"\"\" Get the timestamp (in terms of number of data points) where the majority vote has been achived for an ensmeble of classifiers\n","\n","  Parameters\n","  ----------\n","  timestamps : array\n","    Array of ints with timestamps when each classification is made\n","  predictions : array\n","    Array containing the prediction made by the classifiers at each of the timestamps\n","\n","  Returns\n","  -------\n","    int\n","      The number of data-points after which the majority vote is determined for an ensemble of classifiers \n","  \"\"\"\n","\n","  ## create dict to hold count of predictions\n","  label_counters = defaultdict(int)\n","\n","  ## add entries to dict\n","  for index, pred in enumerate(predictions):\n","    label_counters[pred] += 1\n","\n","    ## if label count == half of total possible predictions then majority is achieved\n","    if(label_counters[pred] == int(len(predictions)/2)+1):\n","      return timestamps[index]\n","  \n","  return -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_T5qTYuPFVA"},"outputs":[],"source":["def get_predictions(number_of_classifiers, ensemble, test_sample, timestamps):\n","  \"\"\" Get final prediction and time to result from neural network ensemble\n","\n","  Parameters\n","  ----------\n","  ensemble : array\n","    Array of trained neural network models\n","  timestamps : array(int)\n","    Array of timestamps at which the predictions are made\n","  test_sample : array\n","    The time series data for the experiement used as the test sample\n","\n","  Returns\n","  -------\n","  final_prediction : double \n","    Final output prediction from the ensmble\n","  time_to_result : int\n","    Time in seconds when the final classification was determined\n","  \"\"\"\n","\n","  pred = []\n","  final_preds = []\n","  \n","  for i in range(number_of_classifiers):\n","    ## create test data that will be predicted by each neural net (will be for every pixel)\n","    test_data = get_test_data_nn(test_sample, timestamps[i])\n","  \n","    ## make prediction for every pixel \n","    en_pred = en[i].predict(test_data)\n","\n","    ## make array of predictions for each pixel (each row is the predictions for one pixel)\n","    if(i==0):\n","      pred = en_pred\n","    else:\n","      pred = np.concatenate((pred, en_pred), axis = 1)\n","    \n","  ## round any value > 0.5 to 1 and < 0.5 to 0 \n","  pred = pred.round()\n","\n","\n","  ## for each classifier count the number of predictions as 1 or 0 and use the max to make final classifier pred as 0 or 1 \n","  for i in range(number_of_classifiers):\n","    classifier_preds = pred[:, i] ## get the predictions made for each pixel by the classifier\n","    final_classifier_pred = Counter(classifier_preds).most_common(1)[0][0] ## get majority pixel vote\n","    final_preds.append(final_classifier_pred) \n","\n","  ## make final prediction for the sample based on majority classifier prediction\n","  final_prediction = Counter(final_preds).most_common(1)[0][0]\n","\n","  ## get ttp from the list of preds made by each classifier\n","  time_index = get_time_index(timestamps, final_preds)\n","  time_to_result = test_sample.T.index[time_index-1] - test_sample.T.index[0]\n","  \n","  return final_prediction, time_to_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPk5gIj2OrNp"},"outputs":[],"source":["## combine positive and negative sample dicts\n","all_samples = {}\n","all_samples.update(negatives)\n","all_samples.update(positives)\n","\n","## create dict of samples with true labels\n","keys = list(all_samples.keys())\n","true_labels = list(np.concatenate((np.zeros(len(negatives)),np.ones(len(positives)))))\n","true_label_dict = dict(zip(keys, true_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FvT0rQlGBCB"},"outputs":[],"source":["final_classifications = {}\n","final_predictions = []\n","final_TTP = []\n","prediction_correctness = []\n","\n","with tf.device(gpu):\n","  for key, value in all_samples.items():\n","    test_sample_name = key\n","    test_sample = value\n","    print(f\"Testing sample: {test_sample_name}...\")\n","\n","    # make ensemble\n","    en = create_ensemble(number_of_classifiers, batch_size, epochs, loss_function, optimiser, timestamps, test_sample_name, positives, negatives)\n","\n","    # get prediction and time to result\n","    sample_classification, time_to_result = get_predictions(number_of_classifiers, en, test_sample, timestamps)\n","\n","    # update arrays with final outcome and time to result\n","    final_predictions.append(sample_classification)\n","    prediction_correctness.append(\"Yes\" if sample_classification == true_label_dict[key] else \"No\")\n","\n","    final_classifications[key] = (sample_classification, true_label_dict[key])\n","    print(f\"Predicted Label: {sample_classification} \\t True Label: {true_label_dict[key]} \\t Correct?: {sample_classification ==true_label_dict[key]} \\n\")\n","\n","    # if final prediction is positive get ttp \n","    if(sample_classification == 1.0):\n","      final_TTP.append(round((time_to_result+30)/60, 2)) # 30s added on because of pre-processing which takes start as 30s after actual start point\n","      print(f\"TTP: {time_to_result + 30}s \\t {round((time_to_result+30)/60, 2)} mins\")\n","    else:\n","      final_TTP.append(np.nan)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhkc8Lnr9-c2"},"outputs":[],"source":["print(f\"Accuracy: {accuracy(final_classifications)}\")\n","print(f\"Sensitivity/Recall: {sensitivity(final_classifications)}\")\n","print(f\"Specificity: {specificity(final_classifications)}\")\n","print(f\"Precision: {precision(final_classifications)}\")\n","print(f\"F1 Score: {f1(final_classifications)}\")"]},{"cell_type":"markdown","source":["#### Confusion Matrix"],"metadata":{"id":"Qgj65jfXBQqS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qf85cGzE6M15"},"outputs":[],"source":["cm = confusion_matrix(true_labels, final_predictions, labels=[0, 1])\n","fig, ax = plt.subplots(1,1,figsize=(7,5))\n","heatmap = sns.heatmap(cm, annot=True, annot_kws={\"size\": 15}, linewidth=0.75, \n","            xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"], cbar=False, cmap='RdPu')\n","\n","heatmap.set_xticklabels(heatmap.get_xmajorticklabels(), fontsize = 15)\n","heatmap.set_yticklabels(heatmap.get_ymajorticklabels(), fontsize = 15)\n","\n","ax.set_ylabel('True Output', fontsize=15, labelpad=15)\n","ax.set_xlabel('Predicted Output', fontsize=15, labelpad=15)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["XVaAULW6qhh1","ttpluWU4tHLq","6cosBM9Jd74f","ihJkU1v2STVo","PaNIFO5iSa9C","Mp6t9KbXUtag","W9kgS_-Cx1nm","KCr7gvB_tf5-","7XgnkewwwPki","j5qgb5mx3rOW"],"machine_shape":"hm","name":"Early Time Series Classification - Pixel Data NN.ipynb","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyO1N7q5jxoyCcfvHMvrGSfg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}